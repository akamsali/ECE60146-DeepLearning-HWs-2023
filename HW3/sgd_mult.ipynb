{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ComputationalGraphPrimer import ComputationalGraphPrimer\n",
    "import operator\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class SGDPlusMultiNeuron(ComputationalGraphPrimer):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def back_prop_modified(self, y_error, class_labels):\n",
    "            \"\"\"\n",
    "            First note that loop index variable 'back_layer_index' starts with the index of\n",
    "            the last layer.  For the 3-layer example shown for 'forward', back_layer_index\n",
    "            starts with a value of 2, its next value is 1, and that's it.\n",
    "\n",
    "            Stochastic Gradient Gradient calls for the backpropagated loss to be averaged over\n",
    "            the samples in a batch.  To explain how this averaging is carried out by the\n",
    "            backprop function, consider the last node on the example shown in the forward()\n",
    "            function above.  Standing at the node, we look at the 'input' values stored in the\n",
    "            variable \"input_vals\".  Assuming a batch size of 8, this will be list of\n",
    "            lists. Each of the inner lists will have two values for the two nodes in the\n",
    "            hidden layer. And there will be 8 of these for the 8 elements of the batch.  We average\n",
    "            these values 'input vals' and store those in the variable \"input_vals_avg\".  Next we\n",
    "            must carry out the same batch-based averaging for the partial derivatives stored in the\n",
    "            variable \"deriv_sigmoid\".\n",
    "\n",
    "            Pay attention to the variable 'vars_in_layer'.  These store the node variables in\n",
    "            the current layer during backpropagation.  Since back_layer_index starts with a\n",
    "            value of 2, the variable 'vars_in_layer' will have just the single node for the\n",
    "            example shown for forward(). With respect to what is stored in vars_in_layer', the\n",
    "            variables stored in 'input_vars_to_layer' correspond to the input layer with\n",
    "            respect to the current layer. \n",
    "            \"\"\"\n",
    "            # backproped prediction error:\n",
    "            pred_err_backproped_at_layers = {i : [] for i in range(1,self.num_layers-1)}  \n",
    "            pred_err_backproped_at_layers[self.num_layers-1] = [y_error]\n",
    "            for back_layer_index in reversed(range(1,self.num_layers)):\n",
    "                input_vals = self.forw_prop_vals_at_layers[back_layer_index -1]\n",
    "                input_vals_avg = [sum(x) for x in zip(*input_vals)]\n",
    "                input_vals_avg = list(map(operator.truediv, input_vals_avg, [float(len(class_labels))] * len(class_labels)))\n",
    "                deriv_sigmoid =  self.gradient_vals_for_layers[back_layer_index]\n",
    "                deriv_sigmoid_avg = [sum(x) for x in zip(*deriv_sigmoid)]\n",
    "                deriv_sigmoid_avg = list(map(operator.truediv, deriv_sigmoid_avg, \n",
    "                                                                [float(len(class_labels))] * len(class_labels)))\n",
    "                vars_in_layer  =  self.layer_vars[back_layer_index]                 ## a list like ['xo']\n",
    "                vars_in_next_layer_back  =  self.layer_vars[back_layer_index - 1]   ## a list like ['xw', 'xz']\n",
    "\n",
    "                layer_params = self.layer_params[back_layer_index]         \n",
    "                ## note that layer_params are stored in a dict like        \n",
    "                    ##     {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
    "                ## \"layer_params[idx]\" is a list of lists for the link weights in layer whose output nodes are in layer \"idx\"\n",
    "                transposed_layer_params = list(zip(*layer_params))         ## creating a transpose of the link matrix\n",
    "\n",
    "                backproped_error = [None] * len(vars_in_next_layer_back)\n",
    "                for k,varr in enumerate(vars_in_next_layer_back):\n",
    "                    for j,var2 in enumerate(vars_in_layer):\n",
    "                        backproped_error[k] = sum([self.vals_for_learnable_params[transposed_layer_params[k][i]] * \n",
    "                                                pred_err_backproped_at_layers[back_layer_index][i] \n",
    "                                                for i in range(len(vars_in_layer))])\n",
    "    #                                               deriv_sigmoid_avg[i] for i in range(len(vars_in_layer))])\n",
    "                pred_err_backproped_at_layers[back_layer_index - 1]  =  backproped_error\n",
    "                input_vars_to_layer = self.layer_vars[back_layer_index-1]\n",
    "                for j,var in enumerate(vars_in_layer):\n",
    "                    layer_params = self.layer_params[back_layer_index][j]\n",
    "                    ##  Regarding the parameter update loop that follows, see the Slides 74 through 77 of my Week 3 \n",
    "                    ##  lecture slides for how the parameters are updated using the partial derivatives stored away \n",
    "                    ##  during forward propagation of data. The theory underlying these calculations is presented \n",
    "                    ##  in Slides 68 through 71. \n",
    "                    for i,param in enumerate(layer_params):\n",
    "                        gradient_of_loss_for_param = input_vals_avg[i] * pred_err_backproped_at_layers[back_layer_index][j]\n",
    "                        # @akamsali: update the velocity parameter and use \n",
    "                        self.v[param] = gradient_of_loss_for_param * deriv_sigmoid_avg[j] \\\n",
    "                                         + (self.momentum * self.v[param])\n",
    "                        \n",
    "                        step = self.learning_rate * self.v[param]\n",
    "                        \n",
    "                        self.vals_for_learnable_params[param] += step\n",
    "                        \n",
    "                self.v_bias[back_layer_index -1] = (self.learning_rate * sum(pred_err_backproped_at_layers[back_layer_index]) \\\n",
    "                                                                * sum(deriv_sigmoid_avg)/len(deriv_sigmoid_avg)) \\\n",
    "                                                                + (self.momentum * self.v_bias[back_layer_index -1])\n",
    "                self.bias[back_layer_index-1] += self.v_bias[back_layer_index -1]\n",
    "        ######################################################################################################\n",
    "    # @akamsali: modified func call name and take in momentum value \\mu\n",
    "    def train(self, training_data, mu=None):\n",
    "\n",
    "\n",
    "        class DataLoader:\n",
    "            def __init__(self, training_data, batch_size):\n",
    "                self.training_data = training_data\n",
    "                self.batch_size = batch_size\n",
    "                self.class_0_samples = [(item, 0) for item in self.training_data[0]]    ## Associate label 0 with each sample\n",
    "                self.class_1_samples = [(item, 1) for item in self.training_data[1]]    ## Associate label 1 with each sample\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.training_data[0]) + len(self.training_data[1])\n",
    "\n",
    "            def _getitem(self):    \n",
    "                cointoss = random.choice([0,1])                            ## When a batch is created by getbatch(), we want the\n",
    "                                                                           ##   samples to be chosen randomly from the two lists\n",
    "                if cointoss == 0:\n",
    "                    return random.choice(self.class_0_samples)\n",
    "                else:\n",
    "                    return random.choice(self.class_1_samples)            \n",
    "\n",
    "            def getbatch(self):\n",
    "                batch_data,batch_labels = [],[]                            ## First list for samples, the second for labels\n",
    "                maxval = 0.0                                               ## For approximate batch data normalization\n",
    "                for _ in range(self.batch_size):\n",
    "                    item = self._getitem()\n",
    "                    if np.max(item[0]) > maxval: \n",
    "                        maxval = np.max(item[0])\n",
    "                    batch_data.append(item[0])\n",
    "                    batch_labels.append(item[1])\n",
    "                batch_data = [item/maxval for item in batch_data]          ## Normalize batch data       \n",
    "                batch = [batch_data, batch_labels]\n",
    "                return batch                \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        The training loop must first initialize the learnable parameters.  Remember, these are the \n",
    "        symbolic names in your input expressions for the neural layer that do not begin with the \n",
    "        letter 'x'.  In this case, we are initializing with random numbers from a uniform distribution \n",
    "        over the interval (0,1).\n",
    "        \"\"\"\n",
    "        self.vals_for_learnable_params = {param: random.uniform(0,1) for param in self.learnable_params}\n",
    "        # @akamsali: initialise v as 0 for learnable parameters\n",
    "        self.v = {param: 0 for param in self.learnable_params}\n",
    "\n",
    "        self.bias = [random.uniform(0,1) for _ in range(self.num_layers-1)]      ## Adding the bias to each layer improves \n",
    "                                                                                 ##   class discrimination. We initialize it \n",
    "                                                                                 ##   to a random number.\n",
    "        # @akamsali: initialise bias velocity to zero \n",
    "        self.v_bias = [0] * (self.num_layers-1)\n",
    "\n",
    "        if mu is None:\n",
    "            self.momentum = 0\n",
    "        else:\n",
    "            self.momentum = mu\n",
    "\n",
    "        data_loader = DataLoader(training_data, batch_size=self.batch_size)\n",
    "        loss_running_record = []\n",
    "        i = 0\n",
    "        avg_loss_over_iterations = 0.0                                          ##  Average the loss over iterations for printing out \n",
    "                                                                                 ##    every N iterations during the training loop.   \n",
    "        for i in range(self.training_iterations):\n",
    "            data = data_loader.getbatch()\n",
    "            data_tuples = data[0]\n",
    "            class_labels = data[1]\n",
    "            self.forward_prop_multi_neuron_model(data_tuples)                                  ## FORW PROP works by side-effect \n",
    "            predicted_labels_for_batch = self.forw_prop_vals_at_layers[self.num_layers-1]      ## Predictions from FORW PROP\n",
    "            y_preds =  [item for sublist in  predicted_labels_for_batch  for item in sublist]  ## Get numeric vals for predictions\n",
    "            loss = sum([(abs(class_labels[i] - y_preds[i]))**2 for i in range(len(class_labels))])  ## Calculate loss for batch\n",
    "            loss_avg = loss / float(len(class_labels))                                         ## Average the loss over batch\n",
    "            avg_loss_over_iterations += loss_avg                                              ## Add to Average loss over iterations\n",
    "            if i%(self.display_loss_how_often) == 0: \n",
    "                avg_loss_over_iterations /= self.display_loss_how_often\n",
    "                loss_running_record.append(avg_loss_over_iterations)\n",
    "                print(\"[iter=%d]  loss = %.4f\" %  (i+1, avg_loss_over_iterations))            ## Display avg loss\n",
    "                avg_loss_over_iterations = 0.0                                                ## Re-initialize avg-over-iterations loss\n",
    "            y_errors = list(map(operator.sub, class_labels, y_preds))\n",
    "            y_error_avg = sum(y_errors) / float(len(class_labels))\n",
    "            # @akamsali: change to modified backprop\n",
    "            self.back_prop_modified(y_error_avg, class_labels)      ## BACKPROP loss\n",
    "            \n",
    "        return loss_running_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "self.layer_expressions:  {1: ['xw=ap*xp+aq*xq+ar*xr+as*xs', 'xz=bp*xp+bq*xq+br*xr+bs*xs'], 2: ['xo=cp*xw+cq*xz']}\n",
      "\n",
      "\n",
      "[layer index: 1] all variables: {'xs', 'xp', 'xw', 'xq', 'xr', 'xz'}\n",
      "\n",
      "\n",
      "[layer index: 1] learnable params: {'as', 'bp', 'ap', 'aq', 'bs', 'ar', 'bq', 'br'}\n",
      "\n",
      "\n",
      "[layer index: 1] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs']}\n",
      "\n",
      "\n",
      "[layer index: 1] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs'}\n",
      "\n",
      "\n",
      "[layer index: 1] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}}\n",
      "\n",
      "\n",
      "[layer index: 1] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5}\n",
      "\n",
      "\n",
      "[layer index: 1] independent vars: {'xp', 'xs', 'xr', 'xq'}\n",
      "\n",
      "\n",
      "[layer index: 1] leads_to dictionary: {'xs': {'xz', 'xw'}, 'xp': {'xz', 'xw'}, 'xw': set(), 'xq': {'xz', 'xw'}, 'xr': {'xz', 'xw'}, 'xz': set()}\n",
      "\n",
      "\n",
      "[layer index: 2] all variables: {'xs', 'xp', 'xo', 'xw', 'xq', 'xr', 'xz'}\n",
      "\n",
      "\n",
      "[layer index: 2] learnable params: {'as', 'bp', 'ap', 'cp', 'aq', 'bs', 'ar', 'cq', 'bq', 'br'}\n",
      "\n",
      "\n",
      "[layer index: 2] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs'], 'xo': ['xw', 'xz']}\n",
      "\n",
      "\n",
      "[layer index: 2] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs', 'xo': 'cp*xw+cq*xz'}\n",
      "\n",
      "\n",
      "[layer index: 2] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}, 'xo': {'xw': 'cp', 'xz': 'cq'}}\n",
      "\n",
      "\n",
      "[layer index: 2] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5, 'xo': 6}\n",
      "\n",
      "\n",
      "[layer index: 2] independent vars: {'xp', 'xs', 'xr', 'xq'}\n",
      "\n",
      "\n",
      "[layer index: 2] leads_to dictionary: {'xs': {'xz', 'xw'}, 'xp': {'xz', 'xw'}, 'xo': set(), 'xw': {'xo'}, 'xq': {'xz', 'xw'}, 'xr': {'xz', 'xw'}, 'xz': {'xo'}}\n",
      "\n",
      "\n",
      "[Final] independent vars: {'xp', 'xs', 'xr', 'xq'}\n",
      "\n",
      "\n",
      "[Final] self.layer_vars:  {0: ['xp', 'xq', 'xr', 'xs'], 1: ['xw', 'xz'], 2: ['xo']}\n",
      "\n",
      "\n",
      "[Final] self.layer_params:  {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
      "\n",
      "\n",
      "[Final] self.layer_exp_objects:  {1: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7f1545c04d00>, <ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7f1544359730>], 2: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7f15442c4490>]}\n"
     ]
    }
   ],
   "source": [
    "cgp = SGDPlusMultiNeuron(\n",
    "               num_layers = 3,\n",
    "               layers_config = [4,2,1],                         # num of nodes in each layer\n",
    "               expressions = ['xw=ap*xp+aq*xq+ar*xr+as*xs',\n",
    "                              'xz=bp*xp+bq*xq+br*xr+bs*xs',\n",
    "                              'xo=cp*xw+cq*xz'],\n",
    "               output_vars = ['xo'],\n",
    "               dataset_size = 5000,\n",
    "               learning_rate = 1e-3,\n",
    "#               learning_rate = 5 * 1e-2,\n",
    "               training_iterations = 40000,\n",
    "               batch_size = 8,\n",
    "               display_loss_how_often = 100,\n",
    "               debug = True,\n",
    "      )\n",
    "\n",
    "cgp.parse_multi_layer_expressions()\n",
    "# cgp.display_multi_neuron_network()   \n",
    "\n",
    "training_data = cgp.gen_training_data()\n",
    "\n",
    "# cgp.run_training_loop_multi_neuron_model( training_data )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter=1]  loss = 0.0034\n",
      "[iter=101]  loss = 0.2674\n",
      "[iter=201]  loss = 0.2704\n",
      "[iter=301]  loss = 0.2668\n",
      "[iter=401]  loss = 0.2656\n",
      "[iter=501]  loss = 0.2582\n",
      "[iter=601]  loss = 0.2620\n",
      "[iter=701]  loss = 0.2677\n",
      "[iter=801]  loss = 0.2655\n",
      "[iter=901]  loss = 0.2613\n",
      "[iter=1001]  loss = 0.2646\n",
      "[iter=1101]  loss = 0.2668\n",
      "[iter=1201]  loss = 0.2654\n",
      "[iter=1301]  loss = 0.2652\n",
      "[iter=1401]  loss = 0.2674\n",
      "[iter=1501]  loss = 0.2620\n",
      "[iter=1601]  loss = 0.2673\n",
      "[iter=1701]  loss = 0.2610\n",
      "[iter=1801]  loss = 0.2679\n",
      "[iter=1901]  loss = 0.2631\n",
      "[iter=2001]  loss = 0.2619\n",
      "[iter=2101]  loss = 0.2603\n",
      "[iter=2201]  loss = 0.2565\n",
      "[iter=2301]  loss = 0.2677\n",
      "[iter=2401]  loss = 0.2611\n",
      "[iter=2501]  loss = 0.2581\n",
      "[iter=2601]  loss = 0.2609\n",
      "[iter=2701]  loss = 0.2577\n",
      "[iter=2801]  loss = 0.2650\n",
      "[iter=2901]  loss = 0.2650\n",
      "[iter=3001]  loss = 0.2569\n",
      "[iter=3101]  loss = 0.2615\n",
      "[iter=3201]  loss = 0.2618\n",
      "[iter=3301]  loss = 0.2568\n",
      "[iter=3401]  loss = 0.2510\n",
      "[iter=3501]  loss = 0.2568\n",
      "[iter=3601]  loss = 0.2594\n",
      "[iter=3701]  loss = 0.2562\n",
      "[iter=3801]  loss = 0.2608\n",
      "[iter=3901]  loss = 0.2582\n",
      "[iter=4001]  loss = 0.2634\n",
      "[iter=4101]  loss = 0.2589\n",
      "[iter=4201]  loss = 0.2567\n",
      "[iter=4301]  loss = 0.2579\n",
      "[iter=4401]  loss = 0.2549\n",
      "[iter=4501]  loss = 0.2579\n",
      "[iter=4601]  loss = 0.2527\n",
      "[iter=4701]  loss = 0.2575\n",
      "[iter=4801]  loss = 0.2541\n",
      "[iter=4901]  loss = 0.2547\n",
      "[iter=5001]  loss = 0.2581\n",
      "[iter=5101]  loss = 0.2534\n",
      "[iter=5201]  loss = 0.2573\n",
      "[iter=5301]  loss = 0.2552\n",
      "[iter=5401]  loss = 0.2549\n",
      "[iter=5501]  loss = 0.2623\n",
      "[iter=5601]  loss = 0.2570\n",
      "[iter=5701]  loss = 0.2558\n",
      "[iter=5801]  loss = 0.2569\n",
      "[iter=5901]  loss = 0.2523\n",
      "[iter=6001]  loss = 0.2558\n",
      "[iter=6101]  loss = 0.2583\n",
      "[iter=6201]  loss = 0.2581\n",
      "[iter=6301]  loss = 0.2565\n",
      "[iter=6401]  loss = 0.2554\n",
      "[iter=6501]  loss = 0.2571\n",
      "[iter=6601]  loss = 0.2529\n",
      "[iter=6701]  loss = 0.2555\n",
      "[iter=6801]  loss = 0.2557\n",
      "[iter=6901]  loss = 0.2502\n",
      "[iter=7001]  loss = 0.2548\n",
      "[iter=7101]  loss = 0.2538\n",
      "[iter=7201]  loss = 0.2560\n",
      "[iter=7301]  loss = 0.2521\n",
      "[iter=7401]  loss = 0.2528\n",
      "[iter=7501]  loss = 0.2548\n",
      "[iter=7601]  loss = 0.2529\n",
      "[iter=7701]  loss = 0.2527\n",
      "[iter=7801]  loss = 0.2564\n",
      "[iter=7901]  loss = 0.2505\n",
      "[iter=8001]  loss = 0.2506\n",
      "[iter=8101]  loss = 0.2521\n",
      "[iter=8201]  loss = 0.2516\n",
      "[iter=8301]  loss = 0.2524\n",
      "[iter=8401]  loss = 0.2565\n",
      "[iter=8501]  loss = 0.2545\n",
      "[iter=8601]  loss = 0.2543\n",
      "[iter=8701]  loss = 0.2533\n",
      "[iter=8801]  loss = 0.2541\n",
      "[iter=8901]  loss = 0.2554\n",
      "[iter=9001]  loss = 0.2535\n",
      "[iter=9101]  loss = 0.2512\n",
      "[iter=9201]  loss = 0.2515\n",
      "[iter=9301]  loss = 0.2495\n",
      "[iter=9401]  loss = 0.2531\n",
      "[iter=9501]  loss = 0.2528\n",
      "[iter=9601]  loss = 0.2521\n",
      "[iter=9701]  loss = 0.2525\n",
      "[iter=9801]  loss = 0.2525\n",
      "[iter=9901]  loss = 0.2508\n",
      "[iter=10001]  loss = 0.2505\n",
      "[iter=10101]  loss = 0.2558\n",
      "[iter=10201]  loss = 0.2518\n",
      "[iter=10301]  loss = 0.2527\n",
      "[iter=10401]  loss = 0.2558\n",
      "[iter=10501]  loss = 0.2506\n",
      "[iter=10601]  loss = 0.2521\n",
      "[iter=10701]  loss = 0.2526\n",
      "[iter=10801]  loss = 0.2512\n",
      "[iter=10901]  loss = 0.2528\n",
      "[iter=11001]  loss = 0.2527\n",
      "[iter=11101]  loss = 0.2520\n",
      "[iter=11201]  loss = 0.2494\n",
      "[iter=11301]  loss = 0.2518\n",
      "[iter=11401]  loss = 0.2520\n",
      "[iter=11501]  loss = 0.2524\n",
      "[iter=11601]  loss = 0.2525\n",
      "[iter=11701]  loss = 0.2535\n",
      "[iter=11801]  loss = 0.2521\n",
      "[iter=11901]  loss = 0.2523\n",
      "[iter=12001]  loss = 0.2508\n",
      "[iter=12101]  loss = 0.2525\n",
      "[iter=12201]  loss = 0.2514\n",
      "[iter=12301]  loss = 0.2498\n",
      "[iter=12401]  loss = 0.2524\n",
      "[iter=12501]  loss = 0.2498\n",
      "[iter=12601]  loss = 0.2536\n",
      "[iter=12701]  loss = 0.2511\n",
      "[iter=12801]  loss = 0.2491\n",
      "[iter=12901]  loss = 0.2512\n",
      "[iter=13001]  loss = 0.2525\n",
      "[iter=13101]  loss = 0.2521\n",
      "[iter=13201]  loss = 0.2518\n",
      "[iter=13301]  loss = 0.2517\n",
      "[iter=13401]  loss = 0.2506\n",
      "[iter=13501]  loss = 0.2521\n",
      "[iter=13601]  loss = 0.2516\n",
      "[iter=13701]  loss = 0.2515\n",
      "[iter=13801]  loss = 0.2504\n",
      "[iter=13901]  loss = 0.2526\n",
      "[iter=14001]  loss = 0.2509\n",
      "[iter=14101]  loss = 0.2514\n",
      "[iter=14201]  loss = 0.2522\n",
      "[iter=14301]  loss = 0.2512\n",
      "[iter=14401]  loss = 0.2509\n",
      "[iter=14501]  loss = 0.2517\n",
      "[iter=14601]  loss = 0.2524\n",
      "[iter=14701]  loss = 0.2514\n",
      "[iter=14801]  loss = 0.2506\n",
      "[iter=14901]  loss = 0.2527\n",
      "[iter=15001]  loss = 0.2516\n",
      "[iter=15101]  loss = 0.2511\n",
      "[iter=15201]  loss = 0.2515\n",
      "[iter=15301]  loss = 0.2525\n",
      "[iter=15401]  loss = 0.2502\n",
      "[iter=15501]  loss = 0.2516\n",
      "[iter=15601]  loss = 0.2518\n",
      "[iter=15701]  loss = 0.2514\n",
      "[iter=15801]  loss = 0.2529\n",
      "[iter=15901]  loss = 0.2517\n",
      "[iter=16001]  loss = 0.2507\n",
      "[iter=16101]  loss = 0.2509\n",
      "[iter=16201]  loss = 0.2515\n",
      "[iter=16301]  loss = 0.2523\n",
      "[iter=16401]  loss = 0.2510\n",
      "[iter=16501]  loss = 0.2514\n",
      "[iter=16601]  loss = 0.2515\n",
      "[iter=16701]  loss = 0.2515\n",
      "[iter=16801]  loss = 0.2513\n",
      "[iter=16901]  loss = 0.2508\n",
      "[iter=17001]  loss = 0.2517\n",
      "[iter=17101]  loss = 0.2518\n",
      "[iter=17201]  loss = 0.2511\n",
      "[iter=17301]  loss = 0.2525\n",
      "[iter=17401]  loss = 0.2515\n",
      "[iter=17501]  loss = 0.2518\n",
      "[iter=17601]  loss = 0.2512\n",
      "[iter=17701]  loss = 0.2514\n",
      "[iter=17801]  loss = 0.2516\n",
      "[iter=17901]  loss = 0.2514\n",
      "[iter=18001]  loss = 0.2509\n",
      "[iter=18101]  loss = 0.2513\n",
      "[iter=18201]  loss = 0.2508\n",
      "[iter=18301]  loss = 0.2512\n",
      "[iter=18401]  loss = 0.2518\n",
      "[iter=18501]  loss = 0.2518\n",
      "[iter=18601]  loss = 0.2512\n",
      "[iter=18701]  loss = 0.2504\n",
      "[iter=18801]  loss = 0.2515\n",
      "[iter=18901]  loss = 0.2509\n",
      "[iter=19001]  loss = 0.2516\n",
      "[iter=19101]  loss = 0.2515\n",
      "[iter=19201]  loss = 0.2511\n",
      "[iter=19301]  loss = 0.2508\n",
      "[iter=19401]  loss = 0.2511\n",
      "[iter=19501]  loss = 0.2518\n",
      "[iter=19601]  loss = 0.2512\n",
      "[iter=19701]  loss = 0.2514\n",
      "[iter=19801]  loss = 0.2517\n",
      "[iter=19901]  loss = 0.2511\n",
      "[iter=20001]  loss = 0.2517\n",
      "[iter=20101]  loss = 0.2516\n",
      "[iter=20201]  loss = 0.2516\n",
      "[iter=20301]  loss = 0.2516\n",
      "[iter=20401]  loss = 0.2515\n",
      "[iter=20501]  loss = 0.2510\n",
      "[iter=20601]  loss = 0.2520\n",
      "[iter=20701]  loss = 0.2510\n",
      "[iter=20801]  loss = 0.2516\n",
      "[iter=20901]  loss = 0.2524\n",
      "[iter=21001]  loss = 0.2506\n",
      "[iter=21101]  loss = 0.2517\n",
      "[iter=21201]  loss = 0.2512\n",
      "[iter=21301]  loss = 0.2519\n",
      "[iter=21401]  loss = 0.2517\n",
      "[iter=21501]  loss = 0.2516\n",
      "[iter=21601]  loss = 0.2512\n",
      "[iter=21701]  loss = 0.2513\n",
      "[iter=21801]  loss = 0.2518\n",
      "[iter=21901]  loss = 0.2517\n",
      "[iter=22001]  loss = 0.2515\n",
      "[iter=22101]  loss = 0.2507\n",
      "[iter=22201]  loss = 0.2517\n",
      "[iter=22301]  loss = 0.2515\n",
      "[iter=22401]  loss = 0.2510\n",
      "[iter=22501]  loss = 0.2518\n",
      "[iter=22601]  loss = 0.2511\n",
      "[iter=22701]  loss = 0.2509\n",
      "[iter=22801]  loss = 0.2516\n",
      "[iter=22901]  loss = 0.2517\n",
      "[iter=23001]  loss = 0.2515\n",
      "[iter=23101]  loss = 0.2515\n",
      "[iter=23201]  loss = 0.2509\n",
      "[iter=23301]  loss = 0.2513\n",
      "[iter=23401]  loss = 0.2510\n",
      "[iter=23501]  loss = 0.2510\n",
      "[iter=23601]  loss = 0.2515\n",
      "[iter=23701]  loss = 0.2514\n",
      "[iter=23801]  loss = 0.2516\n",
      "[iter=23901]  loss = 0.2517\n",
      "[iter=24001]  loss = 0.2522\n",
      "[iter=24101]  loss = 0.2516\n",
      "[iter=24201]  loss = 0.2515\n",
      "[iter=24301]  loss = 0.2522\n",
      "[iter=24401]  loss = 0.2511\n",
      "[iter=24501]  loss = 0.2511\n",
      "[iter=24601]  loss = 0.2511\n",
      "[iter=24701]  loss = 0.2515\n",
      "[iter=24801]  loss = 0.2516\n",
      "[iter=24901]  loss = 0.2517\n",
      "[iter=25001]  loss = 0.2512\n",
      "[iter=25101]  loss = 0.2515\n",
      "[iter=25201]  loss = 0.2513\n",
      "[iter=25301]  loss = 0.2516\n",
      "[iter=25401]  loss = 0.2516\n",
      "[iter=25501]  loss = 0.2519\n",
      "[iter=25601]  loss = 0.2512\n",
      "[iter=25701]  loss = 0.2514\n",
      "[iter=25801]  loss = 0.2512\n",
      "[iter=25901]  loss = 0.2518\n",
      "[iter=26001]  loss = 0.2512\n",
      "[iter=26101]  loss = 0.2512\n",
      "[iter=26201]  loss = 0.2517\n",
      "[iter=26301]  loss = 0.2514\n",
      "[iter=26401]  loss = 0.2521\n",
      "[iter=26501]  loss = 0.2519\n",
      "[iter=26601]  loss = 0.2515\n",
      "[iter=26701]  loss = 0.2514\n",
      "[iter=26801]  loss = 0.2514\n",
      "[iter=26901]  loss = 0.2515\n",
      "[iter=27001]  loss = 0.2512\n",
      "[iter=27101]  loss = 0.2519\n",
      "[iter=27201]  loss = 0.2513\n",
      "[iter=27301]  loss = 0.2517\n",
      "[iter=27401]  loss = 0.2516\n",
      "[iter=27501]  loss = 0.2517\n",
      "[iter=27601]  loss = 0.2513\n",
      "[iter=27701]  loss = 0.2513\n",
      "[iter=27801]  loss = 0.2513\n",
      "[iter=27901]  loss = 0.2516\n",
      "[iter=28001]  loss = 0.2512\n",
      "[iter=28101]  loss = 0.2514\n",
      "[iter=28201]  loss = 0.2516\n",
      "[iter=28301]  loss = 0.2521\n",
      "[iter=28401]  loss = 0.2514\n",
      "[iter=28501]  loss = 0.2514\n",
      "[iter=28601]  loss = 0.2515\n",
      "[iter=28701]  loss = 0.2516\n",
      "[iter=28801]  loss = 0.2518\n",
      "[iter=28901]  loss = 0.2516\n",
      "[iter=29001]  loss = 0.2513\n",
      "[iter=29101]  loss = 0.2517\n",
      "[iter=29201]  loss = 0.2516\n",
      "[iter=29301]  loss = 0.2515\n",
      "[iter=29401]  loss = 0.2512\n",
      "[iter=29501]  loss = 0.2517\n",
      "[iter=29601]  loss = 0.2517\n",
      "[iter=29701]  loss = 0.2515\n",
      "[iter=29801]  loss = 0.2514\n",
      "[iter=29901]  loss = 0.2517\n",
      "[iter=30001]  loss = 0.2519\n",
      "[iter=30101]  loss = 0.2513\n",
      "[iter=30201]  loss = 0.2516\n",
      "[iter=30301]  loss = 0.2516\n",
      "[iter=30401]  loss = 0.2515\n",
      "[iter=30501]  loss = 0.2517\n",
      "[iter=30601]  loss = 0.2514\n",
      "[iter=30701]  loss = 0.2515\n",
      "[iter=30801]  loss = 0.2516\n",
      "[iter=30901]  loss = 0.2515\n",
      "[iter=31001]  loss = 0.2515\n",
      "[iter=31101]  loss = 0.2512\n",
      "[iter=31201]  loss = 0.2516\n",
      "[iter=31301]  loss = 0.2514\n",
      "[iter=31401]  loss = 0.2515\n",
      "[iter=31501]  loss = 0.2516\n",
      "[iter=31601]  loss = 0.2517\n",
      "[iter=31701]  loss = 0.2516\n",
      "[iter=31801]  loss = 0.2516\n",
      "[iter=31901]  loss = 0.2513\n",
      "[iter=32001]  loss = 0.2515\n",
      "[iter=32101]  loss = 0.2518\n",
      "[iter=32201]  loss = 0.2518\n",
      "[iter=32301]  loss = 0.2516\n",
      "[iter=32401]  loss = 0.2516\n",
      "[iter=32501]  loss = 0.2516\n",
      "[iter=32601]  loss = 0.2515\n",
      "[iter=32701]  loss = 0.2516\n",
      "[iter=32801]  loss = 0.2515\n",
      "[iter=32901]  loss = 0.2515\n",
      "[iter=33001]  loss = 0.2516\n",
      "[iter=33101]  loss = 0.2516\n",
      "[iter=33201]  loss = 0.2514\n",
      "[iter=33301]  loss = 0.2516\n",
      "[iter=33401]  loss = 0.2515\n",
      "[iter=33501]  loss = 0.2517\n",
      "[iter=33601]  loss = 0.2517\n",
      "[iter=33701]  loss = 0.2515\n",
      "[iter=33801]  loss = 0.2516\n",
      "[iter=33901]  loss = 0.2517\n",
      "[iter=34001]  loss = 0.2516\n",
      "[iter=34101]  loss = 0.2517\n",
      "[iter=34201]  loss = 0.2515\n",
      "[iter=34301]  loss = 0.2517\n",
      "[iter=34401]  loss = 0.2515\n",
      "[iter=34501]  loss = 0.2516\n",
      "[iter=34601]  loss = 0.2515\n",
      "[iter=34701]  loss = 0.2518\n",
      "[iter=34801]  loss = 0.2516\n",
      "[iter=34901]  loss = 0.2518\n",
      "[iter=35001]  loss = 0.2516\n",
      "[iter=35101]  loss = 0.2517\n",
      "[iter=35201]  loss = 0.2516\n",
      "[iter=35301]  loss = 0.2517\n",
      "[iter=35401]  loss = 0.2517\n",
      "[iter=35501]  loss = 0.2515\n",
      "[iter=35601]  loss = 0.2515\n",
      "[iter=35701]  loss = 0.2517\n",
      "[iter=35801]  loss = 0.2515\n",
      "[iter=35901]  loss = 0.2516\n",
      "[iter=36001]  loss = 0.2516\n",
      "[iter=36101]  loss = 0.2516\n",
      "[iter=36201]  loss = 0.2516\n",
      "[iter=36301]  loss = 0.2516\n",
      "[iter=36401]  loss = 0.2516\n",
      "[iter=36501]  loss = 0.2516\n",
      "[iter=36601]  loss = 0.2516\n",
      "[iter=36701]  loss = 0.2516\n",
      "[iter=36801]  loss = 0.2517\n",
      "[iter=36901]  loss = 0.2517\n",
      "[iter=37001]  loss = 0.2516\n",
      "[iter=37101]  loss = 0.2517\n",
      "[iter=37201]  loss = 0.2516\n",
      "[iter=37301]  loss = 0.2517\n",
      "[iter=37401]  loss = 0.2515\n",
      "[iter=37501]  loss = 0.2516\n",
      "[iter=37601]  loss = 0.2516\n",
      "[iter=37701]  loss = 0.2516\n",
      "[iter=37801]  loss = 0.2516\n",
      "[iter=37901]  loss = 0.2515\n",
      "[iter=38001]  loss = 0.2517\n",
      "[iter=38101]  loss = 0.2516\n",
      "[iter=38201]  loss = 0.2516\n",
      "[iter=38301]  loss = 0.2516\n",
      "[iter=38401]  loss = 0.2516\n",
      "[iter=38501]  loss = 0.2516\n",
      "[iter=38601]  loss = 0.2515\n",
      "[iter=38701]  loss = 0.2516\n",
      "[iter=38801]  loss = 0.2516\n",
      "[iter=38901]  loss = 0.2516\n",
      "[iter=39001]  loss = 0.2516\n",
      "[iter=39101]  loss = 0.2517\n",
      "[iter=39201]  loss = 0.2516\n",
      "[iter=39301]  loss = 0.2516\n",
      "[iter=39401]  loss = 0.2515\n",
      "[iter=39501]  loss = 0.2516\n",
      "[iter=39601]  loss = 0.2515\n",
      "[iter=39701]  loss = 0.2516\n",
      "[iter=39801]  loss = 0.2516\n",
      "[iter=39901]  loss = 0.2515\n",
      "[iter=1]  loss = 0.0028\n",
      "[iter=101]  loss = 0.2571\n",
      "[iter=201]  loss = 0.2621\n",
      "[iter=301]  loss = 0.2574\n",
      "[iter=401]  loss = 0.2531\n",
      "[iter=501]  loss = 0.2554\n",
      "[iter=601]  loss = 0.2546\n",
      "[iter=701]  loss = 0.2538\n",
      "[iter=801]  loss = 0.2573\n",
      "[iter=901]  loss = 0.2545\n",
      "[iter=1001]  loss = 0.2542\n",
      "[iter=1101]  loss = 0.2561\n",
      "[iter=1201]  loss = 0.2546\n",
      "[iter=1301]  loss = 0.2511\n",
      "[iter=1401]  loss = 0.2521\n",
      "[iter=1501]  loss = 0.2546\n",
      "[iter=1601]  loss = 0.2527\n",
      "[iter=1701]  loss = 0.2538\n",
      "[iter=1801]  loss = 0.2534\n",
      "[iter=1901]  loss = 0.2502\n",
      "[iter=2001]  loss = 0.2556\n",
      "[iter=2101]  loss = 0.2546\n",
      "[iter=2201]  loss = 0.2554\n",
      "[iter=2301]  loss = 0.2533\n",
      "[iter=2401]  loss = 0.2511\n",
      "[iter=2501]  loss = 0.2547\n",
      "[iter=2601]  loss = 0.2515\n",
      "[iter=2701]  loss = 0.2481\n",
      "[iter=2801]  loss = 0.2568\n",
      "[iter=2901]  loss = 0.2524\n",
      "[iter=3001]  loss = 0.2508\n",
      "[iter=3101]  loss = 0.2522\n",
      "[iter=3201]  loss = 0.2518\n",
      "[iter=3301]  loss = 0.2493\n",
      "[iter=3401]  loss = 0.2511\n",
      "[iter=3501]  loss = 0.2530\n",
      "[iter=3601]  loss = 0.2499\n",
      "[iter=3701]  loss = 0.2520\n",
      "[iter=3801]  loss = 0.2529\n",
      "[iter=3901]  loss = 0.2526\n",
      "[iter=4001]  loss = 0.2495\n",
      "[iter=4101]  loss = 0.2492\n",
      "[iter=4201]  loss = 0.2519\n",
      "[iter=4301]  loss = 0.2509\n",
      "[iter=4401]  loss = 0.2528\n",
      "[iter=4501]  loss = 0.2517\n",
      "[iter=4601]  loss = 0.2518\n",
      "[iter=4701]  loss = 0.2517\n",
      "[iter=4801]  loss = 0.2518\n",
      "[iter=4901]  loss = 0.2533\n",
      "[iter=5001]  loss = 0.2520\n",
      "[iter=5101]  loss = 0.2520\n",
      "[iter=5201]  loss = 0.2541\n",
      "[iter=5301]  loss = 0.2512\n",
      "[iter=5401]  loss = 0.2503\n",
      "[iter=5501]  loss = 0.2517\n",
      "[iter=5601]  loss = 0.2532\n",
      "[iter=5701]  loss = 0.2507\n",
      "[iter=5801]  loss = 0.2501\n",
      "[iter=5901]  loss = 0.2522\n",
      "[iter=6001]  loss = 0.2504\n",
      "[iter=6101]  loss = 0.2511\n",
      "[iter=6201]  loss = 0.2517\n",
      "[iter=6301]  loss = 0.2502\n",
      "[iter=6401]  loss = 0.2514\n",
      "[iter=6501]  loss = 0.2515\n",
      "[iter=6601]  loss = 0.2520\n",
      "[iter=6701]  loss = 0.2514\n",
      "[iter=6801]  loss = 0.2509\n",
      "[iter=6901]  loss = 0.2507\n",
      "[iter=7001]  loss = 0.2518\n",
      "[iter=7101]  loss = 0.2518\n",
      "[iter=7201]  loss = 0.2510\n",
      "[iter=7301]  loss = 0.2516\n",
      "[iter=7401]  loss = 0.2515\n",
      "[iter=7501]  loss = 0.2515\n",
      "[iter=7601]  loss = 0.2518\n",
      "[iter=7701]  loss = 0.2518\n",
      "[iter=7801]  loss = 0.2523\n",
      "[iter=7901]  loss = 0.2520\n",
      "[iter=8001]  loss = 0.2515\n",
      "[iter=8101]  loss = 0.2508\n",
      "[iter=8201]  loss = 0.2512\n",
      "[iter=8301]  loss = 0.2511\n",
      "[iter=8401]  loss = 0.2510\n",
      "[iter=8501]  loss = 0.2516\n",
      "[iter=8601]  loss = 0.2505\n",
      "[iter=8701]  loss = 0.2513\n",
      "[iter=8801]  loss = 0.2516\n",
      "[iter=8901]  loss = 0.2516\n",
      "[iter=9001]  loss = 0.2511\n",
      "[iter=9101]  loss = 0.2520\n",
      "[iter=9201]  loss = 0.2512\n",
      "[iter=9301]  loss = 0.2516\n",
      "[iter=9401]  loss = 0.2513\n",
      "[iter=9501]  loss = 0.2510\n",
      "[iter=9601]  loss = 0.2517\n",
      "[iter=9701]  loss = 0.2513\n",
      "[iter=9801]  loss = 0.2515\n",
      "[iter=9901]  loss = 0.2521\n",
      "[iter=10001]  loss = 0.2522\n",
      "[iter=10101]  loss = 0.2517\n",
      "[iter=10201]  loss = 0.2515\n",
      "[iter=10301]  loss = 0.2517\n",
      "[iter=10401]  loss = 0.2513\n",
      "[iter=10501]  loss = 0.2517\n",
      "[iter=10601]  loss = 0.2513\n",
      "[iter=10701]  loss = 0.2513\n",
      "[iter=10801]  loss = 0.2515\n",
      "[iter=10901]  loss = 0.2510\n",
      "[iter=11001]  loss = 0.2515\n",
      "[iter=11101]  loss = 0.2520\n",
      "[iter=11201]  loss = 0.2516\n",
      "[iter=11301]  loss = 0.2516\n",
      "[iter=11401]  loss = 0.2516\n",
      "[iter=11501]  loss = 0.2514\n",
      "[iter=11601]  loss = 0.2517\n",
      "[iter=11701]  loss = 0.2516\n",
      "[iter=11801]  loss = 0.2514\n",
      "[iter=11901]  loss = 0.2518\n",
      "[iter=12001]  loss = 0.2514\n",
      "[iter=12101]  loss = 0.2517\n",
      "[iter=12201]  loss = 0.2514\n",
      "[iter=12301]  loss = 0.2516\n",
      "[iter=12401]  loss = 0.2516\n",
      "[iter=12501]  loss = 0.2515\n",
      "[iter=12601]  loss = 0.2514\n",
      "[iter=12701]  loss = 0.2514\n",
      "[iter=12801]  loss = 0.2514\n",
      "[iter=12901]  loss = 0.2518\n",
      "[iter=13001]  loss = 0.2514\n",
      "[iter=13101]  loss = 0.2515\n",
      "[iter=13201]  loss = 0.2515\n",
      "[iter=13301]  loss = 0.2516\n",
      "[iter=13401]  loss = 0.2517\n",
      "[iter=13501]  loss = 0.2516\n",
      "[iter=13601]  loss = 0.2515\n",
      "[iter=13701]  loss = 0.2515\n",
      "[iter=13801]  loss = 0.2513\n",
      "[iter=13901]  loss = 0.2516\n",
      "[iter=14001]  loss = 0.2515\n",
      "[iter=14101]  loss = 0.2515\n",
      "[iter=14201]  loss = 0.2517\n",
      "[iter=14301]  loss = 0.2517\n",
      "[iter=14401]  loss = 0.2518\n",
      "[iter=14501]  loss = 0.2515\n",
      "[iter=14601]  loss = 0.2516\n",
      "[iter=14701]  loss = 0.2516\n",
      "[iter=14801]  loss = 0.2515\n",
      "[iter=14901]  loss = 0.2515\n",
      "[iter=15001]  loss = 0.2515\n",
      "[iter=15101]  loss = 0.2515\n",
      "[iter=15201]  loss = 0.2516\n",
      "[iter=15301]  loss = 0.2516\n",
      "[iter=15401]  loss = 0.2513\n",
      "[iter=15501]  loss = 0.2514\n",
      "[iter=15601]  loss = 0.2516\n",
      "[iter=15701]  loss = 0.2514\n",
      "[iter=15801]  loss = 0.2515\n",
      "[iter=15901]  loss = 0.2514\n",
      "[iter=16001]  loss = 0.2514\n",
      "[iter=16101]  loss = 0.2516\n",
      "[iter=16201]  loss = 0.2512\n",
      "[iter=16301]  loss = 0.2513\n",
      "[iter=16401]  loss = 0.2517\n",
      "[iter=16501]  loss = 0.2513\n",
      "[iter=16601]  loss = 0.2513\n",
      "[iter=16701]  loss = 0.2514\n",
      "[iter=16801]  loss = 0.2515\n",
      "[iter=16901]  loss = 0.2515\n",
      "[iter=17001]  loss = 0.2515\n",
      "[iter=17101]  loss = 0.2516\n",
      "[iter=17201]  loss = 0.2514\n",
      "[iter=17301]  loss = 0.2515\n",
      "[iter=17401]  loss = 0.2512\n",
      "[iter=17501]  loss = 0.2517\n",
      "[iter=17601]  loss = 0.2512\n",
      "[iter=17701]  loss = 0.2514\n",
      "[iter=17801]  loss = 0.2514\n",
      "[iter=17901]  loss = 0.2514\n",
      "[iter=18001]  loss = 0.2514\n",
      "[iter=18101]  loss = 0.2512\n",
      "[iter=18201]  loss = 0.2517\n",
      "[iter=18301]  loss = 0.2519\n",
      "[iter=18401]  loss = 0.2517\n",
      "[iter=18501]  loss = 0.2514\n",
      "[iter=18601]  loss = 0.2514\n",
      "[iter=18701]  loss = 0.2513\n",
      "[iter=18801]  loss = 0.2515\n",
      "[iter=18901]  loss = 0.2516\n",
      "[iter=19001]  loss = 0.2515\n",
      "[iter=19101]  loss = 0.2514\n",
      "[iter=19201]  loss = 0.2514\n",
      "[iter=19301]  loss = 0.2516\n",
      "[iter=19401]  loss = 0.2514\n",
      "[iter=19501]  loss = 0.2517\n",
      "[iter=19601]  loss = 0.2514\n",
      "[iter=19701]  loss = 0.2514\n",
      "[iter=19801]  loss = 0.2514\n",
      "[iter=19901]  loss = 0.2516\n",
      "[iter=20001]  loss = 0.2513\n",
      "[iter=20101]  loss = 0.2516\n",
      "[iter=20201]  loss = 0.2513\n",
      "[iter=20301]  loss = 0.2509\n",
      "[iter=20401]  loss = 0.2515\n",
      "[iter=20501]  loss = 0.2514\n",
      "[iter=20601]  loss = 0.2515\n",
      "[iter=20701]  loss = 0.2511\n",
      "[iter=20801]  loss = 0.2515\n",
      "[iter=20901]  loss = 0.2513\n",
      "[iter=21001]  loss = 0.2515\n",
      "[iter=21101]  loss = 0.2513\n",
      "[iter=21201]  loss = 0.2516\n",
      "[iter=21301]  loss = 0.2518\n",
      "[iter=21401]  loss = 0.2516\n",
      "[iter=21501]  loss = 0.2514\n",
      "[iter=21601]  loss = 0.2514\n",
      "[iter=21701]  loss = 0.2514\n",
      "[iter=21801]  loss = 0.2512\n",
      "[iter=21901]  loss = 0.2517\n",
      "[iter=22001]  loss = 0.2514\n",
      "[iter=22101]  loss = 0.2514\n",
      "[iter=22201]  loss = 0.2514\n",
      "[iter=22301]  loss = 0.2515\n",
      "[iter=22401]  loss = 0.2514\n",
      "[iter=22501]  loss = 0.2515\n",
      "[iter=22601]  loss = 0.2513\n",
      "[iter=22701]  loss = 0.2515\n",
      "[iter=22801]  loss = 0.2514\n",
      "[iter=22901]  loss = 0.2516\n",
      "[iter=23001]  loss = 0.2516\n",
      "[iter=23101]  loss = 0.2515\n",
      "[iter=23201]  loss = 0.2516\n",
      "[iter=23301]  loss = 0.2516\n",
      "[iter=23401]  loss = 0.2512\n",
      "[iter=23501]  loss = 0.2514\n",
      "[iter=23601]  loss = 0.2516\n",
      "[iter=23701]  loss = 0.2513\n",
      "[iter=23801]  loss = 0.2514\n",
      "[iter=23901]  loss = 0.2517\n",
      "[iter=24001]  loss = 0.2513\n",
      "[iter=24101]  loss = 0.2515\n",
      "[iter=24201]  loss = 0.2513\n",
      "[iter=24301]  loss = 0.2514\n",
      "[iter=24401]  loss = 0.2516\n",
      "[iter=24501]  loss = 0.2517\n",
      "[iter=24601]  loss = 0.2515\n",
      "[iter=24701]  loss = 0.2514\n",
      "[iter=24801]  loss = 0.2515\n",
      "[iter=24901]  loss = 0.2517\n",
      "[iter=25001]  loss = 0.2515\n",
      "[iter=25101]  loss = 0.2516\n",
      "[iter=25201]  loss = 0.2516\n",
      "[iter=25301]  loss = 0.2516\n",
      "[iter=25401]  loss = 0.2515\n",
      "[iter=25501]  loss = 0.2516\n",
      "[iter=25601]  loss = 0.2516\n",
      "[iter=25701]  loss = 0.2516\n",
      "[iter=25801]  loss = 0.2516\n",
      "[iter=25901]  loss = 0.2516\n",
      "[iter=26001]  loss = 0.2515\n",
      "[iter=26101]  loss = 0.2515\n",
      "[iter=26201]  loss = 0.2515\n",
      "[iter=26301]  loss = 0.2515\n",
      "[iter=26401]  loss = 0.2515\n",
      "[iter=26501]  loss = 0.2514\n",
      "[iter=26601]  loss = 0.2516\n",
      "[iter=26701]  loss = 0.2514\n",
      "[iter=26801]  loss = 0.2515\n",
      "[iter=26901]  loss = 0.2516\n",
      "[iter=27001]  loss = 0.2516\n",
      "[iter=27101]  loss = 0.2516\n",
      "[iter=27201]  loss = 0.2515\n",
      "[iter=27301]  loss = 0.2515\n",
      "[iter=27401]  loss = 0.2516\n",
      "[iter=27501]  loss = 0.2514\n",
      "[iter=27601]  loss = 0.2517\n",
      "[iter=27701]  loss = 0.2517\n",
      "[iter=27801]  loss = 0.2516\n",
      "[iter=27901]  loss = 0.2517\n",
      "[iter=28001]  loss = 0.2515\n",
      "[iter=28101]  loss = 0.2515\n",
      "[iter=28201]  loss = 0.2515\n",
      "[iter=28301]  loss = 0.2514\n",
      "[iter=28401]  loss = 0.2514\n",
      "[iter=28501]  loss = 0.2515\n",
      "[iter=28601]  loss = 0.2513\n",
      "[iter=28701]  loss = 0.2514\n",
      "[iter=28801]  loss = 0.2515\n",
      "[iter=28901]  loss = 0.2513\n",
      "[iter=29001]  loss = 0.2515\n",
      "[iter=29101]  loss = 0.2514\n",
      "[iter=29201]  loss = 0.2514\n",
      "[iter=29301]  loss = 0.2516\n",
      "[iter=29401]  loss = 0.2515\n",
      "[iter=29501]  loss = 0.2515\n",
      "[iter=29601]  loss = 0.2514\n",
      "[iter=29701]  loss = 0.2515\n",
      "[iter=29801]  loss = 0.2515\n",
      "[iter=29901]  loss = 0.2515\n",
      "[iter=30001]  loss = 0.2515\n",
      "[iter=30101]  loss = 0.2516\n",
      "[iter=30201]  loss = 0.2515\n",
      "[iter=30301]  loss = 0.2515\n",
      "[iter=30401]  loss = 0.2515\n",
      "[iter=30501]  loss = 0.2516\n",
      "[iter=30601]  loss = 0.2514\n",
      "[iter=30701]  loss = 0.2516\n",
      "[iter=30801]  loss = 0.2516\n",
      "[iter=30901]  loss = 0.2516\n",
      "[iter=31001]  loss = 0.2515\n",
      "[iter=31101]  loss = 0.2515\n",
      "[iter=31201]  loss = 0.2514\n",
      "[iter=31301]  loss = 0.2514\n",
      "[iter=31401]  loss = 0.2514\n",
      "[iter=31501]  loss = 0.2513\n",
      "[iter=31601]  loss = 0.2514\n",
      "[iter=31701]  loss = 0.2515\n",
      "[iter=31801]  loss = 0.2514\n",
      "[iter=31901]  loss = 0.2515\n",
      "[iter=32001]  loss = 0.2514\n",
      "[iter=32101]  loss = 0.2514\n",
      "[iter=32201]  loss = 0.2513\n",
      "[iter=32301]  loss = 0.2515\n",
      "[iter=32401]  loss = 0.2513\n",
      "[iter=32501]  loss = 0.2515\n",
      "[iter=32601]  loss = 0.2513\n",
      "[iter=32701]  loss = 0.2514\n",
      "[iter=32801]  loss = 0.2514\n",
      "[iter=32901]  loss = 0.2514\n",
      "[iter=33001]  loss = 0.2514\n",
      "[iter=33101]  loss = 0.2514\n",
      "[iter=33201]  loss = 0.2513\n",
      "[iter=33301]  loss = 0.2512\n",
      "[iter=33401]  loss = 0.2514\n",
      "[iter=33501]  loss = 0.2513\n",
      "[iter=33601]  loss = 0.2512\n",
      "[iter=33701]  loss = 0.2514\n",
      "[iter=33801]  loss = 0.2511\n",
      "[iter=33901]  loss = 0.2513\n",
      "[iter=34001]  loss = 0.2519\n",
      "[iter=34101]  loss = 0.2514\n",
      "[iter=34201]  loss = 0.2515\n",
      "[iter=34301]  loss = 0.2514\n",
      "[iter=34401]  loss = 0.2513\n",
      "[iter=34501]  loss = 0.2513\n",
      "[iter=34601]  loss = 0.2515\n",
      "[iter=34701]  loss = 0.2515\n",
      "[iter=34801]  loss = 0.2514\n",
      "[iter=34901]  loss = 0.2515\n",
      "[iter=35001]  loss = 0.2515\n",
      "[iter=35101]  loss = 0.2514\n",
      "[iter=35201]  loss = 0.2512\n",
      "[iter=35301]  loss = 0.2515\n",
      "[iter=35401]  loss = 0.2511\n",
      "[iter=35501]  loss = 0.2515\n",
      "[iter=35601]  loss = 0.2514\n",
      "[iter=35701]  loss = 0.2514\n",
      "[iter=35801]  loss = 0.2510\n",
      "[iter=35901]  loss = 0.2515\n",
      "[iter=36001]  loss = 0.2513\n",
      "[iter=36101]  loss = 0.2515\n",
      "[iter=36201]  loss = 0.2514\n",
      "[iter=36301]  loss = 0.2515\n",
      "[iter=36401]  loss = 0.2513\n",
      "[iter=36501]  loss = 0.2514\n",
      "[iter=36601]  loss = 0.2512\n",
      "[iter=36701]  loss = 0.2513\n",
      "[iter=36801]  loss = 0.2515\n",
      "[iter=36901]  loss = 0.2514\n",
      "[iter=37001]  loss = 0.2514\n",
      "[iter=37101]  loss = 0.2515\n",
      "[iter=37201]  loss = 0.2515\n",
      "[iter=37301]  loss = 0.2516\n",
      "[iter=37401]  loss = 0.2513\n",
      "[iter=37501]  loss = 0.2513\n",
      "[iter=37601]  loss = 0.2514\n",
      "[iter=37701]  loss = 0.2513\n",
      "[iter=37801]  loss = 0.2513\n",
      "[iter=37901]  loss = 0.2513\n",
      "[iter=38001]  loss = 0.2516\n",
      "[iter=38101]  loss = 0.2514\n",
      "[iter=38201]  loss = 0.2512\n",
      "[iter=38301]  loss = 0.2512\n",
      "[iter=38401]  loss = 0.2513\n",
      "[iter=38501]  loss = 0.2514\n",
      "[iter=38601]  loss = 0.2512\n",
      "[iter=38701]  loss = 0.2514\n",
      "[iter=38801]  loss = 0.2513\n",
      "[iter=38901]  loss = 0.2515\n",
      "[iter=39001]  loss = 0.2514\n",
      "[iter=39101]  loss = 0.2510\n",
      "[iter=39201]  loss = 0.2513\n",
      "[iter=39301]  loss = 0.2513\n",
      "[iter=39401]  loss = 0.2514\n",
      "[iter=39501]  loss = 0.2514\n",
      "[iter=39601]  loss = 0.2514\n",
      "[iter=39701]  loss = 0.2514\n",
      "[iter=39801]  loss = 0.2514\n",
      "[iter=39901]  loss = 0.2514\n",
      "[iter=1]  loss = 0.0023\n",
      "[iter=101]  loss = 0.2783\n",
      "[iter=201]  loss = 0.2676\n",
      "[iter=301]  loss = 0.2710\n",
      "[iter=401]  loss = 0.2562\n",
      "[iter=501]  loss = 0.2619\n",
      "[iter=601]  loss = 0.2535\n",
      "[iter=701]  loss = 0.2558\n",
      "[iter=801]  loss = 0.2556\n",
      "[iter=901]  loss = 0.2552\n",
      "[iter=1001]  loss = 0.2559\n",
      "[iter=1101]  loss = 0.2548\n",
      "[iter=1201]  loss = 0.2521\n",
      "[iter=1301]  loss = 0.2515\n",
      "[iter=1401]  loss = 0.2522\n",
      "[iter=1501]  loss = 0.2522\n",
      "[iter=1601]  loss = 0.2535\n",
      "[iter=1701]  loss = 0.2527\n",
      "[iter=1801]  loss = 0.2526\n",
      "[iter=1901]  loss = 0.2539\n",
      "[iter=2001]  loss = 0.2514\n",
      "[iter=2101]  loss = 0.2523\n",
      "[iter=2201]  loss = 0.2533\n",
      "[iter=2301]  loss = 0.2523\n",
      "[iter=2401]  loss = 0.2530\n",
      "[iter=2501]  loss = 0.2523\n",
      "[iter=2601]  loss = 0.2527\n",
      "[iter=2701]  loss = 0.2527\n",
      "[iter=2801]  loss = 0.2524\n",
      "[iter=2901]  loss = 0.2528\n",
      "[iter=3001]  loss = 0.2527\n",
      "[iter=3101]  loss = 0.2525\n",
      "[iter=3201]  loss = 0.2524\n",
      "[iter=3301]  loss = 0.2524\n",
      "[iter=3401]  loss = 0.2529\n",
      "[iter=3501]  loss = 0.2525\n",
      "[iter=3601]  loss = 0.2525\n",
      "[iter=3701]  loss = 0.2526\n",
      "[iter=3801]  loss = 0.2526\n",
      "[iter=3901]  loss = 0.2524\n",
      "[iter=4001]  loss = 0.2527\n",
      "[iter=4101]  loss = 0.2525\n",
      "[iter=4201]  loss = 0.2525\n",
      "[iter=4301]  loss = 0.2527\n",
      "[iter=4401]  loss = 0.2525\n",
      "[iter=4501]  loss = 0.2528\n",
      "[iter=4601]  loss = 0.2525\n",
      "[iter=4701]  loss = 0.2527\n",
      "[iter=4801]  loss = 0.2526\n",
      "[iter=4901]  loss = 0.2527\n",
      "[iter=5001]  loss = 0.2528\n",
      "[iter=5101]  loss = 0.2528\n",
      "[iter=5201]  loss = 0.2527\n",
      "[iter=5301]  loss = 0.2527\n",
      "[iter=5401]  loss = 0.2528\n",
      "[iter=5501]  loss = 0.2527\n",
      "[iter=5601]  loss = 0.2529\n",
      "[iter=5701]  loss = 0.2528\n",
      "[iter=5801]  loss = 0.2527\n",
      "[iter=5901]  loss = 0.2530\n",
      "[iter=6001]  loss = 0.2526\n",
      "[iter=6101]  loss = 0.2528\n",
      "[iter=6201]  loss = 0.2528\n",
      "[iter=6301]  loss = 0.2522\n",
      "[iter=6401]  loss = 0.2525\n",
      "[iter=6501]  loss = 0.2527\n",
      "[iter=6601]  loss = 0.2525\n",
      "[iter=6701]  loss = 0.2526\n",
      "[iter=6801]  loss = 0.2526\n",
      "[iter=6901]  loss = 0.2525\n",
      "[iter=7001]  loss = 0.2527\n",
      "[iter=7101]  loss = 0.2527\n",
      "[iter=7201]  loss = 0.2526\n",
      "[iter=7301]  loss = 0.2527\n",
      "[iter=7401]  loss = 0.2525\n",
      "[iter=7501]  loss = 0.2523\n",
      "[iter=7601]  loss = 0.2527\n",
      "[iter=7701]  loss = 0.2528\n",
      "[iter=7801]  loss = 0.2524\n",
      "[iter=7901]  loss = 0.2530\n",
      "[iter=8001]  loss = 0.2527\n",
      "[iter=8101]  loss = 0.2527\n",
      "[iter=8201]  loss = 0.2527\n",
      "[iter=8301]  loss = 0.2527\n",
      "[iter=8401]  loss = 0.2524\n",
      "[iter=8501]  loss = 0.2525\n",
      "[iter=8601]  loss = 0.2526\n",
      "[iter=8701]  loss = 0.2524\n",
      "[iter=8801]  loss = 0.2526\n",
      "[iter=8901]  loss = 0.2525\n",
      "[iter=9001]  loss = 0.2526\n",
      "[iter=9101]  loss = 0.2526\n",
      "[iter=9201]  loss = 0.2525\n",
      "[iter=9301]  loss = 0.2524\n",
      "[iter=9401]  loss = 0.2526\n",
      "[iter=9501]  loss = 0.2526\n",
      "[iter=9601]  loss = 0.2526\n",
      "[iter=9701]  loss = 0.2527\n",
      "[iter=9801]  loss = 0.2525\n",
      "[iter=9901]  loss = 0.2526\n",
      "[iter=10001]  loss = 0.2524\n",
      "[iter=10101]  loss = 0.2521\n",
      "[iter=10201]  loss = 0.2523\n",
      "[iter=10301]  loss = 0.2524\n",
      "[iter=10401]  loss = 0.2524\n",
      "[iter=10501]  loss = 0.2525\n",
      "[iter=10601]  loss = 0.2526\n",
      "[iter=10701]  loss = 0.2524\n",
      "[iter=10801]  loss = 0.2526\n",
      "[iter=10901]  loss = 0.2525\n",
      "[iter=11001]  loss = 0.2523\n",
      "[iter=11101]  loss = 0.2522\n",
      "[iter=11201]  loss = 0.2524\n",
      "[iter=11301]  loss = 0.2525\n",
      "[iter=11401]  loss = 0.2526\n",
      "[iter=11501]  loss = 0.2523\n",
      "[iter=11601]  loss = 0.2523\n",
      "[iter=11701]  loss = 0.2524\n",
      "[iter=11801]  loss = 0.2524\n",
      "[iter=11901]  loss = 0.2522\n",
      "[iter=12001]  loss = 0.2526\n",
      "[iter=12101]  loss = 0.2526\n",
      "[iter=12201]  loss = 0.2523\n",
      "[iter=12301]  loss = 0.2521\n",
      "[iter=12401]  loss = 0.2516\n",
      "[iter=12501]  loss = 0.2519\n",
      "[iter=12601]  loss = 0.2527\n",
      "[iter=12701]  loss = 0.2523\n",
      "[iter=12801]  loss = 0.2523\n",
      "[iter=12901]  loss = 0.2522\n",
      "[iter=13001]  loss = 0.2524\n",
      "[iter=13101]  loss = 0.2523\n",
      "[iter=13201]  loss = 0.2523\n",
      "[iter=13301]  loss = 0.2522\n",
      "[iter=13401]  loss = 0.2523\n",
      "[iter=13501]  loss = 0.2522\n",
      "[iter=13601]  loss = 0.2524\n",
      "[iter=13701]  loss = 0.2523\n",
      "[iter=13801]  loss = 0.2522\n",
      "[iter=13901]  loss = 0.2522\n",
      "[iter=14001]  loss = 0.2523\n",
      "[iter=14101]  loss = 0.2523\n",
      "[iter=14201]  loss = 0.2522\n",
      "[iter=14301]  loss = 0.2523\n",
      "[iter=14401]  loss = 0.2519\n",
      "[iter=14501]  loss = 0.2522\n",
      "[iter=14601]  loss = 0.2521\n",
      "[iter=14701]  loss = 0.2522\n",
      "[iter=14801]  loss = 0.2523\n",
      "[iter=14901]  loss = 0.2521\n",
      "[iter=15001]  loss = 0.2522\n",
      "[iter=15101]  loss = 0.2521\n",
      "[iter=15201]  loss = 0.2524\n",
      "[iter=15301]  loss = 0.2521\n",
      "[iter=15401]  loss = 0.2526\n",
      "[iter=15501]  loss = 0.2519\n",
      "[iter=15601]  loss = 0.2522\n",
      "[iter=15701]  loss = 0.2521\n",
      "[iter=15801]  loss = 0.2521\n",
      "[iter=15901]  loss = 0.2525\n",
      "[iter=16001]  loss = 0.2520\n",
      "[iter=16101]  loss = 0.2523\n",
      "[iter=16201]  loss = 0.2522\n",
      "[iter=16301]  loss = 0.2521\n",
      "[iter=16401]  loss = 0.2519\n",
      "[iter=16501]  loss = 0.2520\n",
      "[iter=16601]  loss = 0.2517\n",
      "[iter=16701]  loss = 0.2517\n",
      "[iter=16801]  loss = 0.2520\n",
      "[iter=16901]  loss = 0.2519\n",
      "[iter=17001]  loss = 0.2517\n",
      "[iter=17101]  loss = 0.2525\n",
      "[iter=17201]  loss = 0.2516\n",
      "[iter=17301]  loss = 0.2516\n",
      "[iter=17401]  loss = 0.2519\n",
      "[iter=17501]  loss = 0.2520\n",
      "[iter=17601]  loss = 0.2526\n",
      "[iter=17701]  loss = 0.2521\n",
      "[iter=17801]  loss = 0.2519\n",
      "[iter=17901]  loss = 0.2523\n",
      "[iter=18001]  loss = 0.2523\n",
      "[iter=18101]  loss = 0.2522\n",
      "[iter=18201]  loss = 0.2524\n",
      "[iter=18301]  loss = 0.2523\n",
      "[iter=18401]  loss = 0.2522\n",
      "[iter=18501]  loss = 0.2522\n",
      "[iter=18601]  loss = 0.2524\n",
      "[iter=18701]  loss = 0.2522\n",
      "[iter=18801]  loss = 0.2523\n",
      "[iter=18901]  loss = 0.2522\n",
      "[iter=19001]  loss = 0.2521\n",
      "[iter=19101]  loss = 0.2521\n",
      "[iter=19201]  loss = 0.2523\n",
      "[iter=19301]  loss = 0.2523\n",
      "[iter=19401]  loss = 0.2521\n",
      "[iter=19501]  loss = 0.2523\n",
      "[iter=19601]  loss = 0.2522\n",
      "[iter=19701]  loss = 0.2523\n",
      "[iter=19801]  loss = 0.2520\n",
      "[iter=19901]  loss = 0.2523\n",
      "[iter=20001]  loss = 0.2523\n",
      "[iter=20101]  loss = 0.2522\n",
      "[iter=20201]  loss = 0.2521\n",
      "[iter=20301]  loss = 0.2522\n",
      "[iter=20401]  loss = 0.2521\n",
      "[iter=20501]  loss = 0.2524\n",
      "[iter=20601]  loss = 0.2522\n",
      "[iter=20701]  loss = 0.2522\n",
      "[iter=20801]  loss = 0.2522\n",
      "[iter=20901]  loss = 0.2520\n",
      "[iter=21001]  loss = 0.2520\n",
      "[iter=21101]  loss = 0.2522\n",
      "[iter=21201]  loss = 0.2521\n",
      "[iter=21301]  loss = 0.2523\n",
      "[iter=21401]  loss = 0.2525\n",
      "[iter=21501]  loss = 0.2522\n",
      "[iter=21601]  loss = 0.2519\n",
      "[iter=21701]  loss = 0.2520\n",
      "[iter=21801]  loss = 0.2522\n",
      "[iter=21901]  loss = 0.2520\n",
      "[iter=22001]  loss = 0.2522\n",
      "[iter=22101]  loss = 0.2522\n",
      "[iter=22201]  loss = 0.2520\n",
      "[iter=22301]  loss = 0.2520\n",
      "[iter=22401]  loss = 0.2521\n",
      "[iter=22501]  loss = 0.2519\n",
      "[iter=22601]  loss = 0.2517\n",
      "[iter=22701]  loss = 0.2524\n",
      "[iter=22801]  loss = 0.2524\n",
      "[iter=22901]  loss = 0.2521\n",
      "[iter=23001]  loss = 0.2521\n",
      "[iter=23101]  loss = 0.2521\n",
      "[iter=23201]  loss = 0.2520\n",
      "[iter=23301]  loss = 0.2522\n",
      "[iter=23401]  loss = 0.2520\n",
      "[iter=23501]  loss = 0.2523\n",
      "[iter=23601]  loss = 0.2522\n",
      "[iter=23701]  loss = 0.2520\n",
      "[iter=23801]  loss = 0.2521\n",
      "[iter=23901]  loss = 0.2523\n",
      "[iter=24001]  loss = 0.2521\n",
      "[iter=24101]  loss = 0.2521\n",
      "[iter=24201]  loss = 0.2519\n",
      "[iter=24301]  loss = 0.2521\n",
      "[iter=24401]  loss = 0.2521\n",
      "[iter=24501]  loss = 0.2522\n",
      "[iter=24601]  loss = 0.2520\n",
      "[iter=24701]  loss = 0.2521\n",
      "[iter=24801]  loss = 0.2520\n",
      "[iter=24901]  loss = 0.2521\n",
      "[iter=25001]  loss = 0.2519\n",
      "[iter=25101]  loss = 0.2519\n",
      "[iter=25201]  loss = 0.2519\n",
      "[iter=25301]  loss = 0.2519\n",
      "[iter=25401]  loss = 0.2518\n",
      "[iter=25501]  loss = 0.2521\n",
      "[iter=25601]  loss = 0.2521\n",
      "[iter=25701]  loss = 0.2520\n",
      "[iter=25801]  loss = 0.2520\n",
      "[iter=25901]  loss = 0.2520\n",
      "[iter=26001]  loss = 0.2522\n",
      "[iter=26101]  loss = 0.2520\n",
      "[iter=26201]  loss = 0.2519\n",
      "[iter=26301]  loss = 0.2520\n",
      "[iter=26401]  loss = 0.2519\n",
      "[iter=26501]  loss = 0.2518\n",
      "[iter=26601]  loss = 0.2518\n",
      "[iter=26701]  loss = 0.2519\n",
      "[iter=26801]  loss = 0.2523\n",
      "[iter=26901]  loss = 0.2520\n",
      "[iter=27001]  loss = 0.2517\n",
      "[iter=27101]  loss = 0.2516\n",
      "[iter=27201]  loss = 0.2519\n",
      "[iter=27301]  loss = 0.2517\n",
      "[iter=27401]  loss = 0.2521\n",
      "[iter=27501]  loss = 0.2519\n",
      "[iter=27601]  loss = 0.2517\n",
      "[iter=27701]  loss = 0.2517\n",
      "[iter=27801]  loss = 0.2521\n",
      "[iter=27901]  loss = 0.2519\n",
      "[iter=28001]  loss = 0.2518\n",
      "[iter=28101]  loss = 0.2520\n",
      "[iter=28201]  loss = 0.2515\n",
      "[iter=28301]  loss = 0.2515\n",
      "[iter=28401]  loss = 0.2518\n",
      "[iter=28501]  loss = 0.2521\n",
      "[iter=28601]  loss = 0.2518\n",
      "[iter=28701]  loss = 0.2518\n",
      "[iter=28801]  loss = 0.2515\n",
      "[iter=28901]  loss = 0.2517\n",
      "[iter=29001]  loss = 0.2519\n",
      "[iter=29101]  loss = 0.2518\n",
      "[iter=29201]  loss = 0.2517\n",
      "[iter=29301]  loss = 0.2518\n",
      "[iter=29401]  loss = 0.2517\n",
      "[iter=29501]  loss = 0.2514\n",
      "[iter=29601]  loss = 0.2516\n",
      "[iter=29701]  loss = 0.2516\n",
      "[iter=29801]  loss = 0.2517\n",
      "[iter=29901]  loss = 0.2514\n",
      "[iter=30001]  loss = 0.2522\n",
      "[iter=30101]  loss = 0.2521\n",
      "[iter=30201]  loss = 0.2517\n",
      "[iter=30301]  loss = 0.2519\n",
      "[iter=30401]  loss = 0.2515\n",
      "[iter=30501]  loss = 0.2518\n",
      "[iter=30601]  loss = 0.2521\n",
      "[iter=30701]  loss = 0.2518\n",
      "[iter=30801]  loss = 0.2520\n",
      "[iter=30901]  loss = 0.2521\n",
      "[iter=31001]  loss = 0.2518\n",
      "[iter=31101]  loss = 0.2518\n",
      "[iter=31201]  loss = 0.2518\n",
      "[iter=31301]  loss = 0.2517\n",
      "[iter=31401]  loss = 0.2519\n",
      "[iter=31501]  loss = 0.2519\n",
      "[iter=31601]  loss = 0.2518\n",
      "[iter=31701]  loss = 0.2519\n",
      "[iter=31801]  loss = 0.2519\n",
      "[iter=31901]  loss = 0.2522\n",
      "[iter=32001]  loss = 0.2519\n",
      "[iter=32101]  loss = 0.2521\n",
      "[iter=32201]  loss = 0.2519\n",
      "[iter=32301]  loss = 0.2518\n",
      "[iter=32401]  loss = 0.2518\n",
      "[iter=32501]  loss = 0.2521\n",
      "[iter=32601]  loss = 0.2517\n",
      "[iter=32701]  loss = 0.2518\n",
      "[iter=32801]  loss = 0.2517\n",
      "[iter=32901]  loss = 0.2518\n",
      "[iter=33001]  loss = 0.2518\n",
      "[iter=33101]  loss = 0.2518\n",
      "[iter=33201]  loss = 0.2516\n",
      "[iter=33301]  loss = 0.2518\n",
      "[iter=33401]  loss = 0.2517\n",
      "[iter=33501]  loss = 0.2517\n",
      "[iter=33601]  loss = 0.2517\n",
      "[iter=33701]  loss = 0.2517\n",
      "[iter=33801]  loss = 0.2515\n",
      "[iter=33901]  loss = 0.2517\n",
      "[iter=34001]  loss = 0.2515\n",
      "[iter=34101]  loss = 0.2516\n",
      "[iter=34201]  loss = 0.2515\n",
      "[iter=34301]  loss = 0.2509\n",
      "[iter=34401]  loss = 0.2518\n",
      "[iter=34501]  loss = 0.2518\n",
      "[iter=34601]  loss = 0.2517\n",
      "[iter=34701]  loss = 0.2516\n",
      "[iter=34801]  loss = 0.2517\n",
      "[iter=34901]  loss = 0.2517\n",
      "[iter=35001]  loss = 0.2516\n",
      "[iter=35101]  loss = 0.2518\n",
      "[iter=35201]  loss = 0.2516\n",
      "[iter=35301]  loss = 0.2518\n",
      "[iter=35401]  loss = 0.2518\n",
      "[iter=35501]  loss = 0.2517\n",
      "[iter=35601]  loss = 0.2517\n",
      "[iter=35701]  loss = 0.2515\n",
      "[iter=35801]  loss = 0.2518\n",
      "[iter=35901]  loss = 0.2516\n",
      "[iter=36001]  loss = 0.2515\n",
      "[iter=36101]  loss = 0.2515\n",
      "[iter=36201]  loss = 0.2515\n",
      "[iter=36301]  loss = 0.2517\n",
      "[iter=36401]  loss = 0.2516\n",
      "[iter=36501]  loss = 0.2517\n",
      "[iter=36601]  loss = 0.2517\n",
      "[iter=36701]  loss = 0.2515\n",
      "[iter=36801]  loss = 0.2516\n",
      "[iter=36901]  loss = 0.2516\n",
      "[iter=37001]  loss = 0.2517\n",
      "[iter=37101]  loss = 0.2516\n",
      "[iter=37201]  loss = 0.2515\n",
      "[iter=37301]  loss = 0.2514\n",
      "[iter=37401]  loss = 0.2515\n",
      "[iter=37501]  loss = 0.2517\n",
      "[iter=37601]  loss = 0.2516\n",
      "[iter=37701]  loss = 0.2516\n",
      "[iter=37801]  loss = 0.2517\n",
      "[iter=37901]  loss = 0.2516\n",
      "[iter=38001]  loss = 0.2517\n",
      "[iter=38101]  loss = 0.2514\n",
      "[iter=38201]  loss = 0.2512\n",
      "[iter=38301]  loss = 0.2517\n",
      "[iter=38401]  loss = 0.2515\n",
      "[iter=38501]  loss = 0.2514\n",
      "[iter=38601]  loss = 0.2517\n",
      "[iter=38701]  loss = 0.2515\n",
      "[iter=38801]  loss = 0.2514\n",
      "[iter=38901]  loss = 0.2510\n",
      "[iter=39001]  loss = 0.2515\n",
      "[iter=39101]  loss = 0.2512\n",
      "[iter=39201]  loss = 0.2517\n",
      "[iter=39301]  loss = 0.2515\n",
      "[iter=39401]  loss = 0.2509\n",
      "[iter=39501]  loss = 0.2515\n",
      "[iter=39601]  loss = 0.2512\n",
      "[iter=39701]  loss = 0.2518\n",
      "[iter=39801]  loss = 0.2516\n",
      "[iter=39901]  loss = 0.2513\n"
     ]
    }
   ],
   "source": [
    "loss_0 = cgp.train(training_data)\n",
    "loss_5 = cgp.train(training_data, mu=0.5)\n",
    "loss_9 = cgp.train(training_data, mu=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_0, label=0)\n",
    "plt.plot(loss_5, label='0.5')\n",
    "plt.plot(loss_9, label='0.9')\n",
    "plt.legend()\n",
    "plt.title(\"SGD+ training with varying momenta\")\n",
    "plt.ylabel(\"Running loss\")\n",
    "plt.xlabel(\"Averaged intervals \")\n",
    "plt.savefig('sgdp_059.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece60146",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:35) \n[GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f680f4d97cf514679e00d4da1d9b5ffad508e5bcade52d18cdbe0b82edcbe4b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
