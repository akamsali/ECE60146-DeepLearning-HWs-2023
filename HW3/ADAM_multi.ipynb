{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ComputationalGraphPrimer import ComputationalGraphPrimer\n",
    "import operator\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class myADAMMultiNeuron(ComputationalGraphPrimer):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def backprop_and_update_params_multineuron(self, y_error, class_labels):\n",
    "            # backproped prediction error:\n",
    "            pred_err_backproped_at_layers = {i : [] for i in range(1,self.num_layers-1)}  \n",
    "            pred_err_backproped_at_layers[self.num_layers-1] = [y_error]\n",
    "            for back_layer_index in reversed(range(1,self.num_layers)):\n",
    "                input_vals = self.forw_prop_vals_at_layers[back_layer_index -1]\n",
    "                input_vals_avg = [sum(x) for x in zip(*input_vals)]\n",
    "                input_vals_avg = list(map(operator.truediv, input_vals_avg, [float(len(class_labels))] * len(class_labels)))\n",
    "                deriv_sigmoid =  self.gradient_vals_for_layers[back_layer_index]\n",
    "                deriv_sigmoid_avg = [sum(x) for x in zip(*deriv_sigmoid)]\n",
    "                deriv_sigmoid_avg = list(map(operator.truediv, deriv_sigmoid_avg, \n",
    "                                                                [float(len(class_labels))] * len(class_labels)))\n",
    "                vars_in_layer  =  self.layer_vars[back_layer_index]                 ## a list like ['xo']\n",
    "                vars_in_next_layer_back  =  self.layer_vars[back_layer_index - 1]   ## a list like ['xw', 'xz']\n",
    "\n",
    "                layer_params = self.layer_params[back_layer_index]         \n",
    "                ## note that layer_params are stored in a dict like        \n",
    "                    ##     {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
    "                ## \"layer_params[idx]\" is a list of lists for the link weights in layer whose output nodes are in layer \"idx\"\n",
    "                transposed_layer_params = list(zip(*layer_params))         ## creating a transpose of the link matrix\n",
    "\n",
    "                backproped_error = [None] * len(vars_in_next_layer_back)\n",
    "                for k,varr in enumerate(vars_in_next_layer_back):\n",
    "                    for j,var2 in enumerate(vars_in_layer):\n",
    "                        backproped_error[k] = sum([self.vals_for_learnable_params[transposed_layer_params[k][i]] * \n",
    "                                                pred_err_backproped_at_layers[back_layer_index][i] \n",
    "                                                for i in range(len(vars_in_layer))])\n",
    "    #                                               deriv_sigmoid_avg[i] for i in range(len(vars_in_layer))])\n",
    "                pred_err_backproped_at_layers[back_layer_index - 1]  =  backproped_error\n",
    "                input_vars_to_layer = self.layer_vars[back_layer_index-1]\n",
    "                for j,var in enumerate(vars_in_layer):\n",
    "                    layer_params = self.layer_params[back_layer_index][j]\n",
    "                    ##  Regarding the parameter update loop that follows, see the Slides 74 through 77 of my Week 3 \n",
    "                    ##  lecture slides for how the parameters are updated using the partial derivatives stored away \n",
    "                    ##  during forward propagation of data. The theory underlying these calculations is presented \n",
    "                    ##  in Slides 68 through 71. \n",
    "                    for i,param in enumerate(layer_params):\n",
    "                        \n",
    "                        # @akamsali: update the velocity parameter and use \n",
    "                        g_t = input_vals_avg[i] * pred_err_backproped_at_layers[back_layer_index][j] * deriv_sigmoid_avg[j] \n",
    "\n",
    "                        m_val = self.beta_1 * self.m[param] + (1-self.beta_1) * g_t\n",
    "                        v_val = self.beta_2 * self.v[param] + (1-self.beta_2) * (g_t**2)\n",
    "                        m_hat = m_val / (1 - self.beta_1 ** self.time[param] )\n",
    "                        v_hat = v_val / (1 - self.beta_2 ** self.time[param] )\n",
    "\n",
    "                        ## Update the learnable parameters\n",
    "                        step = self.learning_rate * m_hat / np.sqrt(v_hat + self.epsilon)\n",
    "                        self.vals_for_learnable_params[param] += step\n",
    "                        # store the current values of first and second moment parameters \n",
    "                        # for next iteration of training\n",
    "                        self.m[param] = m_val\n",
    "                        self.v[param] = v_val\n",
    "                        self.time[param] += 1 # update time step \n",
    "\n",
    "                ## Update the bias\n",
    "                m_bias_val = self.beta_1 * self.m_bias[back_layer_index -1] + \\\n",
    "                            (1 - self.beta_1) * (np.sum(pred_err_backproped_at_layers[back_layer_index]) * np.mean(deriv_sigmoid_avg))\n",
    "                v_bias_val = self.beta_2 * self.v_bias[back_layer_index -1] + \\\n",
    "                            (1 - self.beta_2) * (np.sum(pred_err_backproped_at_layers[back_layer_index]) * np.mean(deriv_sigmoid_avg)**2)\n",
    "\n",
    "                m_bias_hat = m_bias_val / (1 - (self.beta_1 ** self.time_bias[back_layer_index -1]))\n",
    "                v_bias_hat = v_bias_val / (1 - (self.beta_2 ** self.time_bias[back_layer_index -1]))\n",
    "                \n",
    "                ## Update the bias parameters\n",
    "                bias_step = self.learning_rate * (m_bias_hat / np.sqrt(v_bias_hat + 1e-7)) \n",
    "                self.bias += bias_step\n",
    "\n",
    "                # store the current values of first and second moment parameters \n",
    "                # for next iteration of training\n",
    "\n",
    "                self.m_bias[back_layer_index -1] = m_bias_val\n",
    "                self.v_bias[back_layer_index -1] = v_bias_val\n",
    "                self.time_bias[back_layer_index -1] += 1 # update time step \n",
    "                        \n",
    "\n",
    "    ######################################################################################################\n",
    "    # @akamsali: modified func call name and take in momentum value \\mu\n",
    "    def train_multineuron(self, training_data, beta_1=0.9, beta_2=0.99, epsilon=1e-7):\n",
    "\n",
    "\n",
    "        class DataLoader:\n",
    "            def __init__(self, training_data, batch_size):\n",
    "                self.training_data = training_data\n",
    "                self.batch_size = batch_size\n",
    "                self.class_0_samples = [(item, 0) for item in self.training_data[0]]    ## Associate label 0 with each sample\n",
    "                self.class_1_samples = [(item, 1) for item in self.training_data[1]]    ## Associate label 1 with each sample\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.training_data[0]) + len(self.training_data[1])\n",
    "\n",
    "            def _getitem(self):    \n",
    "                cointoss = random.choice([0,1])                            ## When a batch is created by getbatch(), we want the\n",
    "                                                                           ##   samples to be chosen randomly from the two lists\n",
    "                if cointoss == 0:\n",
    "                    return random.choice(self.class_0_samples)\n",
    "                else:\n",
    "                    return random.choice(self.class_1_samples)            \n",
    "\n",
    "            def getbatch(self):\n",
    "                batch_data,batch_labels = [],[]                            ## First list for samples, the second for labels\n",
    "                maxval = 0.0                                               ## For approximate batch data normalization\n",
    "                for _ in range(self.batch_size):\n",
    "                    item = self._getitem()\n",
    "                    if np.max(item[0]) > maxval: \n",
    "                        maxval = np.max(item[0])\n",
    "                    batch_data.append(item[0])\n",
    "                    batch_labels.append(item[1])\n",
    "                batch_data = [item/maxval for item in batch_data]          ## Normalize batch data       \n",
    "                batch = [batch_data, batch_labels]\n",
    "                return batch                \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        The training loop must first initialize the learnable parameters.  Remember, these are the \n",
    "        symbolic names in your input expressions for the neural layer that do not begin with the \n",
    "        letter 'x'.  In this case, we are initializing with random numbers from a uniform distribution \n",
    "        over the interval (0,1).\n",
    "        \"\"\"\n",
    "        self.vals_for_learnable_params = {param: random.uniform(0,1) for param in self.learnable_params}\n",
    "\n",
    "\n",
    "        self.bias = [random.uniform(0,1) for _ in range(self.num_layers-1)]      ## Adding the bias to each layer improves \n",
    "                                                                                 ##   class discrimination. We initialize it \n",
    "                                                                                 ##   to a random number.\n",
    "        # @akamsali: set hyperparameters\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon \n",
    "        # @akamsali: initialise learnable parameter moments\n",
    "        self.m = {param: 0 for param in self.learnable_params}\n",
    "        self.v = {param: 0 for param in self.learnable_params}\n",
    "        self.time = {param: 1 for param in self.learnable_params}\n",
    "\n",
    "        # @akamsali: initialise bias parameter moments\n",
    "        self.time_bias = [1]*(self.num_layers-1)\n",
    "        self.m_bias = [0]*(self.num_layers-1)\n",
    "        self.v_bias = [0]*(self.num_layers-1)\n",
    "\n",
    "\n",
    "        data_loader = DataLoader(training_data, batch_size=self.batch_size)\n",
    "        loss_running_record = []\n",
    "        i = 0\n",
    "        avg_loss_over_iterations = 0.0                                          ##  Average the loss over iterations for printing out \n",
    "                                                                                 ##    every N iterations during the training loop.   \n",
    "        for i in tqdm(range(self.training_iterations)):\n",
    "            data = data_loader.getbatch()\n",
    "            data_tuples = data[0]\n",
    "            print(data_tuples)\n",
    "            class_labels = data[1]\n",
    "            self.forward_prop_multi_neuron_model(data_tuples)                                  ## FORW PROP works by side-effect \n",
    "            predicted_labels_for_batch = self.forw_prop_vals_at_layers[self.num_layers-1]      ## Predictions from FORW PROP\n",
    "            y_preds =  [item for sublist in  predicted_labels_for_batch  for item in sublist]  ## Get numeric vals for predictions\n",
    "            loss = sum([(abs(class_labels[i] - y_preds[i]))**2 for i in range(len(class_labels))])  ## Calculate loss for batch\n",
    "            loss_avg = loss / float(len(class_labels))                                         ## Average the loss over batch\n",
    "            avg_loss_over_iterations += loss_avg                                              ## Add to Average loss over iterations\n",
    "            if i%(self.display_loss_how_often) == 0: \n",
    "                avg_loss_over_iterations /= self.display_loss_how_often\n",
    "                loss_running_record.append(avg_loss_over_iterations)\n",
    "                # print(\"[iter=%d]  loss = %.4f\" %  (i+1, avg_loss_over_iterations))            ## Display avg loss\n",
    "                avg_loss_over_iterations = 0.0                                                ## Re-initialize avg-over-iterations loss\n",
    "            y_errors = list(map(operator.sub, class_labels, y_preds))\n",
    "            y_error_avg = sum(y_errors) / float(len(class_labels))\n",
    "            # @akamsali: change to modified backprop\n",
    "            self.backprop_and_update_params_multineuron(y_error_avg, class_labels)      ## BACKPROP loss\n",
    "            \n",
    "        return loss_running_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "self.layer_expressions:  {1: ['xw=ap*xp+aq*xq+ar*xr+as*xs', 'xz=bp*xp+bq*xq+br*xr+bs*xs'], 2: ['xo=cp*xw+cq*xz']}\n",
      "\n",
      "\n",
      "[layer index: 1] all variables: {'xz', 'xw', 'xr', 'xq', 'xs', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 1] learnable params: {'as', 'br', 'ap', 'bs', 'bq', 'aq', 'bp', 'ar'}\n",
      "\n",
      "\n",
      "[layer index: 1] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs']}\n",
      "\n",
      "\n",
      "[layer index: 1] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs'}\n",
      "\n",
      "\n",
      "[layer index: 1] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}}\n",
      "\n",
      "\n",
      "[layer index: 1] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5}\n",
      "\n",
      "\n",
      "[layer index: 1] independent vars: {'xs', 'xr', 'xq', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 1] leads_to dictionary: {'xz': set(), 'xw': set(), 'xr': {'xz', 'xw'}, 'xq': {'xz', 'xw'}, 'xs': {'xz', 'xw'}, 'xp': {'xz', 'xw'}}\n",
      "\n",
      "\n",
      "[layer index: 2] all variables: {'xz', 'xw', 'xr', 'xo', 'xq', 'xs', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 2] learnable params: {'as', 'br', 'ap', 'cq', 'bs', 'bq', 'aq', 'cp', 'bp', 'ar'}\n",
      "\n",
      "\n",
      "[layer index: 2] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs'], 'xo': ['xw', 'xz']}\n",
      "\n",
      "\n",
      "[layer index: 2] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs', 'xo': 'cp*xw+cq*xz'}\n",
      "\n",
      "\n",
      "[layer index: 2] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}, 'xo': {'xw': 'cp', 'xz': 'cq'}}\n",
      "\n",
      "\n",
      "[layer index: 2] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5, 'xo': 6}\n",
      "\n",
      "\n",
      "[layer index: 2] independent vars: {'xs', 'xr', 'xq', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 2] leads_to dictionary: {'xz': {'xo'}, 'xw': {'xo'}, 'xr': {'xz', 'xw'}, 'xo': set(), 'xq': {'xz', 'xw'}, 'xs': {'xz', 'xw'}, 'xp': {'xz', 'xw'}}\n",
      "\n",
      "\n",
      "[Final] independent vars: {'xs', 'xr', 'xq', 'xp'}\n",
      "\n",
      "\n",
      "[Final] self.layer_vars:  {0: ['xp', 'xq', 'xr', 'xs'], 1: ['xw', 'xz'], 2: ['xo']}\n",
      "\n",
      "\n",
      "[Final] self.layer_params:  {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
      "\n",
      "\n",
      "[Final] self.layer_exp_objects:  {1: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7fcaf159f310>, <ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7fcaf159f730>], 2: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x7fcaf159f670>]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40000 [00:00<?, ?it/s]/var/folders/j4/wb_q1s_s6cl43n5d2r4t8p3w0000gn/T/ipykernel_8672/1691721402.py:76: RuntimeWarning: invalid value encountered in sqrt\n",
      "  bias_step = self.learning_rate * (m_bias_hat / np.sqrt(v_bias_hat + 1e-7))\n",
      "  0%|          | 1/40000 [00:00<02:02, 326.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.26613241, 0.33472789, 0.26284519, 0.47481621]), array([0.41121133, 0.07398863, 0.20280787, 0.33127024]), array([0.27224541, 0.3150424 , 0.49610528, 0.26374297]), array([0.34846216, 0.19218335, 0.25382971, 0.38505873]), array([0.54688986, 0.42842752, 0.1464999 , 0.53522788]), array([-0.05895718,  0.27288617,  0.43104638,  0.28889614]), array([1.        , 0.65034497, 0.14605088, 0.22077299]), array([0.60953372, 0.81883191, 0.41011074, 0.18566385])]\n",
      "[array([ 0.29262678,  0.40141971, -0.04441873,  0.45633414]), array([0.52018342, 0.56687619, 0.14056941, 0.3893314 ]), array([0.13334504, 0.24188899, 0.34890361, 0.30680977]), array([0.45145664, 0.35476537, 0.28351984, 0.76326755]), array([0.39009914, 0.56129811, 0.34963711, 0.2480258 ]), array([0.38225309, 0.12145688, 0.0381678 , 0.54185796]), array([0.40850117, 0.35623534, 0.32909018, 0.40955579]), array([1.        , 0.68887441, 0.3806984 , 0.48415363])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m adam_mn\u001b[39m.\u001b[39mparse_multi_layer_expressions()\n\u001b[1;32m     19\u001b[0m training_data \u001b[39m=\u001b[39m adam_mn\u001b[39m.\u001b[39mgen_training_data()\n\u001b[0;32m---> 20\u001b[0m sgd_mn_loss_0 \u001b[39m=\u001b[39m adam_mn\u001b[39m.\u001b[39;49mtrain_multineuron(training_data)\n\u001b[1;32m     21\u001b[0m \u001b[39m# sgd_mn_loss_5 = adam_mn.train(training_data, mu=0.5)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# sgd_mn_loss_9 = adam_mn.train(training_data, mu=0.9)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 161\u001b[0m, in \u001b[0;36mmyADAMMultiNeuron.train_multineuron\u001b[0;34m(self, training_data, beta_1, beta_2, epsilon)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mprint\u001b[39m(data_tuples)\n\u001b[1;32m    160\u001b[0m class_labels \u001b[39m=\u001b[39m data[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 161\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_prop_multi_neuron_model(data_tuples)                                  \u001b[39m## FORW PROP works by side-effect \u001b[39;00m\n\u001b[1;32m    162\u001b[0m predicted_labels_for_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforw_prop_vals_at_layers[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]      \u001b[39m## Predictions from FORW PROP\u001b[39;00m\n\u001b[1;32m    163\u001b[0m y_preds \u001b[39m=\u001b[39m  [item \u001b[39mfor\u001b[39;00m sublist \u001b[39min\u001b[39;00m  predicted_labels_for_batch  \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m sublist]  \u001b[39m## Get numeric vals for predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Acads/ECE60146/ComputationalGraphPrimer-1.1.2/ComputationalGraphPrimer/ComputationalGraphPrimer.py:1568\u001b[0m, in \u001b[0;36mComputationalGraphPrimer.forward_prop_multi_neuron_model\u001b[0;34m(self, data_tuples_in_batch)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[39m##  In the following loop for forward propagation calculations, exp_obj is the Exp object\u001b[39;00m\n\u001b[1;32m   1565\u001b[0m \u001b[39m##  that is created for each user-supplied expression that specifies the network.  See the\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m \u001b[39m##  definition of the class Exp (for 'Expression') by searching for \"class Exp\":\u001b[39;00m\n\u001b[1;32m   1567\u001b[0m \u001b[39mfor\u001b[39;00m exp_obj \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_exp_objects[layer_index]:\n\u001b[0;32m-> 1568\u001b[0m     output_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_expression(exp_obj\u001b[39m.\u001b[39;49mbody , vals_for_input_vars_dict,    \n\u001b[1;32m   1569\u001b[0m                                                  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvals_for_learnable_params, input_vars)\n\u001b[1;32m   1570\u001b[0m     \u001b[39m## [Search for \"self.bias\" in this file.]  As mentioned earlier, adding bias to each \u001b[39;00m\n\u001b[1;32m   1571\u001b[0m     \u001b[39m##  layer improves class discrimination:\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m     output_val \u001b[39m=\u001b[39m output_val \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias[layer_index\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]                \n",
      "File \u001b[0;32m~/Documents/Acads/ECE60146/ComputationalGraphPrimer-1.1.2/ComputationalGraphPrimer/ComputationalGraphPrimer.py:2185\u001b[0m, in \u001b[0;36mComputationalGraphPrimer.eval_expression\u001b[0;34m(self, exp, vals_for_vars, vals_for_learnable_params, ind_vars)\u001b[0m\n\u001b[1;32m   2183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug1:                     \n\u001b[1;32m   2184\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mSTEP4: [replaced learnable params by their vals] exp: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m exp)\n\u001b[0;32m-> 2185\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39meval\u001b[39;49m( exp\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m^\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m**\u001b[39;49m\u001b[39m'\u001b[39;49m) )\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nan' is not defined"
     ]
    }
   ],
   "source": [
    "adam_mn = myADAMMultiNeuron(\n",
    "               num_layers = 3,\n",
    "               layers_config = [4,2,1],                         # num of nodes in each layer\n",
    "               expressions = ['xw=ap*xp+aq*xq+ar*xr+as*xs',\n",
    "                              'xz=bp*xp+bq*xq+br*xr+bs*xs',\n",
    "                              'xo=cp*xw+cq*xz'],\n",
    "               output_vars = ['xo'],\n",
    "               dataset_size = 5000,\n",
    "               learning_rate = 1e-3,\n",
    "#               learning_rate = 5 * 1e-2,\n",
    "               training_iterations = 40000,\n",
    "               batch_size = 8,\n",
    "               display_loss_how_often = 100,\n",
    "               debug = True,\n",
    "      )\n",
    "\n",
    "adam_mn.parse_multi_layer_expressions()\n",
    "\n",
    "training_data = adam_mn.gen_training_data()\n",
    "sgd_mn_loss_0 = adam_mn.train_multineuron(training_data)\n",
    "# sgd_mn_loss_5 = adam_mn.train(training_data, mu=0.5)\n",
    "# sgd_mn_loss_9 = adam_mn.train(training_data, mu=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece60146",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9e2c8e2c3eec83fdd947ae0c01953e9312087244dd81f250c0b41cce591b6de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
