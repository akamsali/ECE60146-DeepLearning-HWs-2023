{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@akamsali:\n",
    "modified from the backprop_and_update_params_one_neuron_model method in the ComputationalGraphPrimer class\n",
    "in the ComputationalGraphPrimer.py file.\n",
    "\n",
    "The modification is to use the SGD+ algorithm to update the step size\n",
    "\n",
    "from:\n",
    "\n",
    "$p_{t+1} = p_t - \\eta g_{t+1}$\n",
    "\n",
    "to:\n",
    "\n",
    "$m_{t+1} = \\beta_1 * m_t + (1-\\beta_1) * g_{t+1}$\n",
    "\n",
    "$v_{t+1} = \\beta_2 * v_t + (1-\\beta_2) * g_{t+1}^2$\n",
    "\n",
    "$p_{t+1} = p_t - \\eta (\\hat{m}_{t+1} / \\sqrt{\\hat{v}_{t+1}})$\n",
    "\n",
    "$\\hat{m}_{t+1} = m_{t}/(1 - \\beta_1^t)$\n",
    "\n",
    "$\\hat{v}_{t+1} = v_{t}/(1 - \\beta_2^t)$\n",
    "\n",
    "\n",
    "where m and v are the first and second momentum parameters\n",
    "\n",
    "\\eta = learning rate\n",
    "\n",
    "$g_{t+1}$ = gradient of the loss function with respect to the learnable parameters (input val * deriv_sigmoid)\n",
    "\n",
    "$p_t$ = learnable parameters\n",
    "\n",
    "$m_0$ = all 0's\n",
    "\n",
    "$v_0$ = all 0's\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ComputationalGraphPrimer import ComputationalGraphPrimer\n",
    "import random\n",
    "import numpy as np\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "\n",
    "class myADAM(ComputationalGraphPrimer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def backprop_and_update_parama_bias(self, y_error, vals_for_input_vars, deriv_sigmoid):\n",
    "        \"\"\"\n",
    "  \n",
    "        \"\"\"\n",
    "\n",
    "        input_vars = self.independent_vars\n",
    "        vals_for_input_vars_dict = dict(zip(input_vars, list(vals_for_input_vars)))\n",
    "        vals_for_learnable_params = self.vals_for_learnable_params\n",
    "        for i, param in enumerate(self.vals_for_learnable_params):\n",
    "            ## Calculate the next step in the parameter hyperplane\n",
    "            g_t = y_error * vals_for_input_vars_dict[input_vars[i]] * deriv_sigmoid\n",
    "            m_val = self.beta_1 * self.m[param] + (1-self.beta_1) * g_t\n",
    "            v_val = self.beta_2 * self.v[param] + (1-self.beta_2) * (g_t**2)\n",
    "            m_hat = m_val / (1 - self.beta_1 ** self.time[param] )\n",
    "            v_hat = v_val / (1 - self.beta_2 ** self.time[param] )\n",
    "\n",
    "            step = self.learning_rate * m_hat / np.sqrt(v_hat + self.epsilon)\n",
    "            ## Update the learnable parameters\n",
    "            self.vals_for_learnable_params[param] += step\n",
    "            self.m[param] = m_val\n",
    "            self.v[param] = v_val\n",
    "            self.time[param] += 1\n",
    "\n",
    "        ## Update the bias\n",
    "        m_bias_val = self.beta1 * self.m_bias + (1 - self.beta1) * (y_error * deriv_sigmoid)\n",
    "        v_bias_val = self.beta2 * self.v_bias + (1 - self.beta2) * ((y_error * deriv_sigmoid)**2)\n",
    "\n",
    "        m_bias_hat = m_bias_val / (1 - (beta1 ** self.time[-1]))\n",
    "        v_bias_hat = v_bias_val / (1 - (beta2 ** self.time[-1]))\n",
    "        bias_step = self.learning_rate * (m_bias_hat / np.sqrt(v_bias_hat + 1e-7)) \n",
    "        \n",
    "        self.bias += bias_step\n",
    "        self.m_bias = m_bias_val\n",
    "        self.v_bias = v_bias_val\n",
    "\n",
    "\n",
    "    def train(self, training_data, beta_1=0.9, beta_2=0.99, epsilon=1e-7):\n",
    "        \"\"\"\n",
    "        @akamsali: Taking Avi's code as is for training a one neuron model.  The only modification\n",
    "        is to return the loss running record so that we can plot it later.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        The training loop must first initialize the learnable parameters.  Remember, these are the \n",
    "        symbolic names in your input expressions for the neural layer that do not begin with the \n",
    "        letter 'x'.  In this case, we are initializing with random numbers from a uniform distribution \n",
    "        over the interval (0,1).\n",
    "        \"\"\"\n",
    "\n",
    "        self.vals_for_learnable_params = {\n",
    "            param: random.uniform(0, 1) for param in self.learnable_params\n",
    "        }\n",
    "\n",
    "        self.bias = random.uniform(\n",
    "            0, 1\n",
    "        )  ## Adding the bias improves class discrimination.\n",
    "        ##   We initialize it to a random number.\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon \n",
    "        self.m = {param: 0 for param in self.learnable_params}\n",
    "        self.v = {param: 0 for param in self.learnable_params}\n",
    "        self.time = {param: 1 for param in self.learnable_params}\n",
    "\n",
    "        self.m_bias = 0\n",
    "        self.v_bias = 0\n",
    "\n",
    "        class DataLoader:\n",
    "            \"\"\"\n",
    "            To understand the logic of the dataloader, it would help if you first understand how\n",
    "            the training dataset is created.  Search for the following function in this file:\n",
    "\n",
    "                             gen_training_data(self)\n",
    "\n",
    "            As you will see in the implementation code for this method, the training dataset\n",
    "            consists of a Python dict with two keys, 0 and 1, the former points to a list of\n",
    "            all Class 0 samples and the latter to a list of all Class 1 samples.  In each list,\n",
    "            the data samples are drawn from a multi-dimensional Gaussian distribution.  The two\n",
    "            classes have different means and variances.  The dimensionality of each data sample\n",
    "            is set by the number of nodes in the input layer of the neural network.\n",
    "\n",
    "            The data loader's job is to construct a batch of samples drawn randomly from the two\n",
    "            lists mentioned above.  And it mush also associate the class label with each sample\n",
    "            separately.\n",
    "            \"\"\"\n",
    "\n",
    "            def __init__(self, training_data, batch_size):\n",
    "                self.training_data = training_data\n",
    "                self.batch_size = batch_size\n",
    "                self.class_0_samples = [\n",
    "                    (item, 0) for item in self.training_data[0]\n",
    "                ]  ## Associate label 0 with each sample\n",
    "                self.class_1_samples = [\n",
    "                    (item, 1) for item in self.training_data[1]\n",
    "                ]  ## Associate label 1 with each sample\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.training_data[0]) + len(self.training_data[1])\n",
    "\n",
    "            def _getitem(self):\n",
    "                cointoss = random.choice(\n",
    "                    [0, 1]\n",
    "                )  ## When a batch is created by getbatch(), we want the\n",
    "                ##   samples to be chosen randomly from the two lists\n",
    "                if cointoss == 0:\n",
    "                    return random.choice(self.class_0_samples)\n",
    "                else:\n",
    "                    return random.choice(self.class_1_samples)\n",
    "\n",
    "            def getbatch(self):\n",
    "                batch_data, batch_labels = (\n",
    "                    [],\n",
    "                    [],\n",
    "                )  ## First list for samples, the second for labels\n",
    "                maxval = 0.0  ## For approximate batch data normalization\n",
    "                for _ in range(self.batch_size):\n",
    "                    item = self._getitem()\n",
    "                    if np.max(item[0]) > maxval:\n",
    "                        maxval = np.max(item[0])\n",
    "                    batch_data.append(item[0])\n",
    "                    batch_labels.append(item[1])\n",
    "                batch_data = [\n",
    "                    item / maxval for item in batch_data\n",
    "                ]  ## Normalize batch data\n",
    "                batch = [batch_data, batch_labels]\n",
    "                return batch\n",
    "\n",
    "        data_loader = DataLoader(training_data, batch_size=self.batch_size)\n",
    "        loss_running_record = []\n",
    "        i = 0\n",
    "        avg_loss_over_iterations = (\n",
    "            0.0  ##  Average the loss over iterations for printing out\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        ##    every N iterations during the training loop.\n",
    "        for i in range(self.training_iterations):\n",
    "            data = data_loader.getbatch()\n",
    "            data_tuples = data[0]\n",
    "            class_labels = data[1]\n",
    "            y_preds, deriv_sigmoids = self.forward_prop_one_neuron_model(\n",
    "                data_tuples\n",
    "            )  ##  FORWARD PROP of data\n",
    "            loss = sum(\n",
    "                [\n",
    "                    (abs(class_labels[i] - y_preds[i])) ** 2\n",
    "                    for i in range(len(class_labels))\n",
    "                ]\n",
    "            )  ##  Find loss\n",
    "            loss_avg = loss / float(len(class_labels))  ##  Average the loss over batch\n",
    "            avg_loss_over_iterations += loss_avg\n",
    "            if i % (self.display_loss_how_often) == 0:\n",
    "                avg_loss_over_iterations /= self.display_loss_how_often\n",
    "                loss_running_record.append(avg_loss_over_iterations)\n",
    "                # print(\"[iter=%d]  loss = %.4f\" %  (i+1, avg_loss_over_iterations))                 ## Display average loss\n",
    "                avg_loss_over_iterations = 0.0  ## Re-initialize avg loss\n",
    "            y_errors = list(map(operator.sub, class_labels, y_preds))\n",
    "            y_error_avg = sum(y_errors) / float(len(class_labels))\n",
    "            deriv_sigmoid_avg = sum(deriv_sigmoids) / float(len(class_labels))\n",
    "            data_tuple_avg = [sum(x) for x in zip(*data_tuples)]\n",
    "            data_tuple_avg = list(\n",
    "                map(\n",
    "                    operator.truediv,\n",
    "                    data_tuple_avg,\n",
    "                    [float(len(class_labels))] * len(class_labels),\n",
    "                )\n",
    "            )\n",
    "            self.backprop_and_update_parama_bias(\n",
    "                y_error_avg, data_tuple_avg, deriv_sigmoid_avg\n",
    "            )  ## BACKPROP loss\n",
    "        # plt.figure()\n",
    "        return loss_running_record\n",
    "        # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece60146",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f680f4d97cf514679e00d4da1d9b5ffad508e5bcade52d18cdbe0b82edcbe4b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
