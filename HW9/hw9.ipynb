{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ViTHelper import MasterEncoder, BasicEncoder, SelfAttention\n",
    "\n",
    "import torch\n",
    "import torchsummary\n",
    "master_encoder = MasterEncoder(max_seq_length=17, embedding_size=16*16*3, how_many_basic_encoders=1, num_atten_heads=2)\n",
    "test = torch.rand(1, 17, 16*16*3)\n",
    "encoder = BasicEncoder(max_seq_length=17, embedding_size=16*16*3, num_atten_heads=2)\n",
    "self_attn = SelfAttention(max_seq_length=17, embedding_size=16*16*3, num_atten_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1              [-1, 17, 768]           1,536\n",
      "            Linear-2                 [-1, 6528]      42,621,312\n",
      "            Linear-3                 [-1, 6528]      42,621,312\n",
      "            Linear-4                 [-1, 6528]      42,621,312\n",
      "           Softmax-5               [-1, 17, 17]               0\n",
      "     AttentionHead-6              [-1, 17, 384]               0\n",
      "            Linear-7                 [-1, 6528]      42,621,312\n",
      "            Linear-8                 [-1, 6528]      42,621,312\n",
      "            Linear-9                 [-1, 6528]      42,621,312\n",
      "          Softmax-10               [-1, 17, 17]               0\n",
      "    AttentionHead-11              [-1, 17, 384]               0\n",
      "    SelfAttention-12              [-1, 17, 768]               0\n",
      "        LayerNorm-13              [-1, 17, 768]           1,536\n",
      "           Linear-14                [-1, 26112]     340,944,384\n",
      "           Linear-15                [-1, 13056]     340,931,328\n",
      "================================================================\n",
      "Total params: 937,606,656\n",
      "Trainable params: 937,606,656\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 1.00\n",
      "Params size (MB): 3576.69\n",
      "Estimated Total Size (MB): 3577.74\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 6528]      42,621,312\n",
      "            Linear-2                 [-1, 6528]      42,621,312\n",
      "            Linear-3                 [-1, 6528]      42,621,312\n",
      "           Softmax-4               [-1, 17, 17]               0\n",
      "     AttentionHead-5              [-1, 17, 384]               0\n",
      "            Linear-6                 [-1, 6528]      42,621,312\n",
      "            Linear-7                 [-1, 6528]      42,621,312\n",
      "            Linear-8                 [-1, 6528]      42,621,312\n",
      "           Softmax-9               [-1, 17, 17]               0\n",
      "    AttentionHead-10              [-1, 17, 384]               0\n",
      "================================================================\n",
      "Total params: 255,727,872\n",
      "Trainable params: 255,727,872\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 0.40\n",
      "Params size (MB): 975.52\n",
      "Estimated Total Size (MB): 975.98\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1              [-1, 17, 768]           1,536\n",
      "            Linear-2                 [-1, 6528]      42,621,312\n",
      "            Linear-3                 [-1, 6528]      42,621,312\n",
      "            Linear-4                 [-1, 6528]      42,621,312\n",
      "           Softmax-5               [-1, 17, 17]               0\n",
      "     AttentionHead-6              [-1, 17, 384]               0\n",
      "            Linear-7                 [-1, 6528]      42,621,312\n",
      "            Linear-8                 [-1, 6528]      42,621,312\n",
      "            Linear-9                 [-1, 6528]      42,621,312\n",
      "          Softmax-10               [-1, 17, 17]               0\n",
      "    AttentionHead-11              [-1, 17, 384]               0\n",
      "    SelfAttention-12              [-1, 17, 768]               0\n",
      "        LayerNorm-13              [-1, 17, 768]           1,536\n",
      "           Linear-14                [-1, 26112]     340,944,384\n",
      "           Linear-15                [-1, 13056]     340,931,328\n",
      "     BasicEncoder-16              [-1, 17, 768]               0\n",
      "================================================================\n",
      "Total params: 937,606,656\n",
      "Trainable params: 937,606,656\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 1.10\n",
      "Params size (MB): 3576.69\n",
      "Estimated Total Size (MB): 3577.84\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(torchsummary.summary(encoder, (17, 16*16*3)))\n",
    "print(torchsummary.summary(self_attn, (17, 16*16*3)))\n",
    "print(torchsummary.summary(master_encoder, (17, 16*16*3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece60146",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
