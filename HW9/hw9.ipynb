{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model...\n",
      "creating basic encoder...\n",
      "creating self attention layer...\n",
      "creating basic encoder...\n",
      "creating self attention layer...\n",
      "creating self attention layer...\n"
     ]
    }
   ],
   "source": [
    "from ViTHelper import MasterEncoder, BasicEncoder, SelfAttention, AttentionHead\n",
    "\n",
    "import torch\n",
    "import torchsummary\n",
    "master_encoder = MasterEncoder(max_seq_length=17, embedding_size=8, how_many_basic_encoders=1, num_atten_heads=2)\n",
    "# test = torch.rand(1, 17, 16*16*3)\n",
    "encoder = BasicEncoder(max_seq_length=17, embedding_size=2*2*3, num_atten_heads=1)\n",
    "self_attn = SelfAttention(max_seq_length=17, embedding_size=2*2*3, num_atten_heads=3)\n",
    "attn_head = AttentionHead(max_seq_length=17, qkv_size=2*2*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1                [-1, 17, 8]              16\n",
      "            Linear-2                   [-1, 68]           4,692\n",
      "            Linear-3                   [-1, 68]           4,692\n",
      "            Linear-4                   [-1, 68]           4,692\n",
      "           Softmax-5               [-1, 17, 17]               0\n",
      "     AttentionHead-6                [-1, 17, 4]               0\n",
      "            Linear-7                   [-1, 68]           4,692\n",
      "            Linear-8                   [-1, 68]           4,692\n",
      "            Linear-9                   [-1, 68]           4,692\n",
      "          Softmax-10               [-1, 17, 17]               0\n",
      "    AttentionHead-11                [-1, 17, 4]               0\n",
      "    SelfAttention-12                [-1, 17, 8]               0\n",
      "        LayerNorm-13                [-1, 17, 8]              16\n",
      "           Linear-14                  [-1, 272]          37,264\n",
      "           Linear-15                  [-1, 136]          37,128\n",
      "     BasicEncoder-16                [-1, 17, 8]               0\n",
      "================================================================\n",
      "Total params: 102,576\n",
      "Trainable params: 102,576\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 0.41\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print(torchsummary.summary(attn_head, (17, 2*2*3)))\n",
    "# print(torchsummary.summary(self_attn, (17, 2*2*3)))\n",
    "# print(torchsummary.summary(encoder, (17, 2*2*3)))\n",
    "print(torchsummary.summary(master_encoder, ( 17, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model...\n",
      "creating basic encoder...\n",
      "creating self attention layer...\n",
      "torch.Size([1, 17, 8])\n"
     ]
    }
   ],
   "source": [
    "master_encoder = MasterEncoder(max_seq_length=17, embedding_size=8, how_many_basic_encoders=1, num_atten_heads=2)\n",
    "test = torch.rand(1, 17, 8)\n",
    "print(master_encoder(test).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "torch.Size([1, 1, 8]) torch.Size([2, 1, 8]) torch.Size([2, 16, 8])\n",
      "torch.Size([2, 17, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 17, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "class ViTEmbeddings(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, num_classes, embedding_size):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        print(self.num_patches)\n",
    "        self.patch_embedding = nn.Conv2d(3, embedding_size, kernel_size=patch_size, stride=patch_size)\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, self.num_patches + 1, embedding_size))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embedding_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        # flatten  using einops\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        print(self.cls_token.shape, self.cls_token.expand(x.size(0), -1, -1).shape, x.shape)\n",
    "        x = torch.cat((self.cls_token.expand(x.size(0), -1, -1), x), dim=1)\n",
    "        x = x + self.positional_embedding\n",
    "        return x\n",
    "    \n",
    "vit_embeddings = ViTEmbeddings(img_size=64, patch_size=16, num_classes=5, embedding_size=8)\n",
    "test = torch.rand(2, 3, 64, 64)\n",
    "# test.backward()\n",
    "out = vit_embeddings(test)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "# from self_attention_cv import TransformerEncoder\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                 img_dim,\n",
    "                 in_channels=3,\n",
    "                 patch_dim=16,\n",
    "                 num_classes=10,\n",
    "                 dim=512,\n",
    "                 blocks=6,\n",
    "                 heads=4,\n",
    "                 dim_linear_block=1024,\n",
    "                 dim_head=None,\n",
    "                 dropout=0, transformer=None, classification=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dim: the spatial image size\n",
    "            in_channels: number of img channels\n",
    "            patch_dim: desired patch dim\n",
    "            num_classes: classification task classes\n",
    "            dim: the linear layer's dim to project the patches for MHSA\n",
    "            blocks: number of transformer blocks\n",
    "            heads: number of heads\n",
    "            dim_linear_block: inner dim of the transformer linear block\n",
    "            dim_head: dim head in case you want to define it. defaults to dim/heads\n",
    "            dropout: for pos emb and transformer\n",
    "            transformer: in case you want to provide another transformer implementation\n",
    "            classification: creates an extra CLS token\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert img_dim % patch_dim == 0, f'patch size {patch_dim} not divisible'\n",
    "        self.p = patch_dim\n",
    "        tokens = (img_dim // patch_dim) ** 2\n",
    "        self.token_dim = in_channels * (patch_dim ** 2)\n",
    "        self.dim = dim\n",
    "        self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
    "        self.project_patches = nn.Linear(self.token_dim, dim)\n",
    "\n",
    "        self.emb_dropout = nn.Dropout(dropout)\n",
    "        if self.classification:\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "            self.pos_emb1D = nn.Parameter(torch.randn(tokens + 1, dim))\n",
    "            self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        else:\n",
    "            self.pos_emb1D = nn.Parameter(torch.randn(tokens, dim))\n",
    "\n",
    "        # if transformer is None:\n",
    "        #     self.transformer = TransformerEncoder(dim, blocks=blocks, heads=heads,\n",
    "        #                                           dim_head=self.dim_head,\n",
    "        #                                           dim_linear_block=dim_linear_block,\n",
    "        #                                           dropout=dropout)\n",
    "        # else:\n",
    "        #     self.transformer = transformer\n",
    "\n",
    "    def expand_cls_to_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: batch size\n",
    "        Returns: cls token expanded to the batch size\n",
    "        \"\"\"\n",
    "        return self.cls_token.expand([batch, -1, -1])\n",
    "\n",
    "    def forward(self, img, mask=None):\n",
    "        batch_size = img.shape[0]\n",
    "        img_patches = rearrange(\n",
    "            img, 'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
    "                                patch_x=self.p, patch_y=self.p)\n",
    "        # project patches with linear layer + add pos emb\n",
    "        img_patches = self.project_patches(img_patches)\n",
    "\n",
    "        if self.classification:\n",
    "            img_patches = torch.cat(\n",
    "                (self.expand_cls_to_batch(batch_size), img_patches), dim=1)\n",
    "\n",
    "        patch_embeddings = self.emb_dropout(img_patches + self.pos_emb1D)\n",
    "\n",
    "        # feed patch_embeddings and output of transformer. shape: [batch, tokens, dim]\n",
    "        y = self.transformer(patch_embeddings, mask)\n",
    "\n",
    "        if self.classification:\n",
    "            # we index only the cls token for classification. nlp tricks :P\n",
    "            return self.mlp_head(y[:, 0, :])\n",
    "        else:\n",
    "            return y\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece60146",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
