{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/DLStudio/DLStudio.py:5231: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1634272482218/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  review_tensor = torch.FloatTensor( list_of_embeddings )\n",
      "/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/DLStudio/DLStudio.py:5231: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1634272482218/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  review_tensor = torch.FloatTensor( list_of_embeddings )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/j4/wb_q1s_s6cl43n5d2r4t8p3w0000gn/T/ipykernel_32132/1387191538.py\", line 9, in <module>\n",
      "    train.train(net, name, data_size=400, lr=1e-5, momentum=0.9, epochs=1)\n",
      "  File \"/Users/akshita/Documents/Acads/ECE60146/HW8/train.py\", line 82, in train\n",
      "    output = net(review_tensor)\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/akshita/Documents/Acads/ECE60146/HW8/model.py\", line 94, in forward\n",
      "TypeError: 'module' object is not callable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/akshita/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# from train import Train\n",
    "# from model import RNN_custom\n",
    "# dataroot = \"/Users/akshita/Documents/Acads/data/sentiment_analysis/\"\n",
    "# path_to_saved_embeddings = \"/Users/akshita/Documents/Acads/data/word2vec\"\n",
    "\n",
    "# train = Train(dataroot, path_to_saved_embeddings=path_to_saved_embeddings)\n",
    "# net = RNN_custom(300, 512, 2, True, 2)\n",
    "# name = \"RNN_custom\"\n",
    "# train.train(net, name, data_size=400, lr=1e-5, momentum=0.9, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLStudio import DLStudio\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn as nn \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"/Users/akshita/Documents/Acads/data/sentiment_analysis/\"\n",
    "dataset_archive_train = \"sentiment_dataset_train_400.tar.gz\"\n",
    "dataset_archive_test = \"sentiment_dataset_test_400.tar.gz\"\n",
    "path_to_saved_embeddings = \"/Users/akshita/Documents/Acads/data/word2vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRU(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size, \n",
    "                 bias=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: size of input vectors\n",
    "            hidden_size: size of hidden state vectors\n",
    "            bias: whether to use bias parameters or not\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, 3 * hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize all weights uniformly in the range [-1/sqrt(n), 1/sqrt(n)]\n",
    "        # n = hidden_size\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: of shape (batch_size, input_size)\n",
    "            hx: of shape (batch_size, hidden_size)\n",
    "\n",
    "        Returns:    \n",
    "            hy: of shape (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        if hx is None:\n",
    "            hx = Variable(input.new_zeros(input.size(0), self.hidden_size))\n",
    "\n",
    "        # Compute x_t and h_t\n",
    "        x_t = self.x2h(input)\n",
    "        h_t = self.h2h(hx)\n",
    "\n",
    "        # we split the output of the linear layers into 3 parts\n",
    "        # each of size hidden_size\n",
    "        x_reset, x_upd, x_new = x_t.chunk(3, 1)\n",
    "        h_reset, h_upd, h_new = h_t.chunk(3, 1)\n",
    "\n",
    "        # compute the reset, update and new gates\n",
    "        reset_gate = torch.sigmoid(x_reset + h_reset)\n",
    "        update_gate = torch.sigmoid(x_upd + h_upd)\n",
    "        new_gate = torch.tanh(x_new + (reset_gate * h_new))\n",
    "\n",
    "        hy = update_gate * hx + (1 - update_gate) * new_gate\n",
    "\n",
    "        return hy\n",
    "    \n",
    "class RNN_custom(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size,\n",
    "                 num_layers, \n",
    "                 bias,\n",
    "                output_size\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # list of GRU cells\n",
    "        self.rnn_list = nn.ModuleList()\n",
    "        self.rnn_list.append(MyGRU(input_size, \n",
    "                                     hidden_size, \n",
    "                                     bias))\n",
    "        for i in range(num_layers-1):\n",
    "            self.rnn_list.append(MyGRU(hidden_size, \n",
    "                                         hidden_size, \n",
    "                                         bias))\n",
    "        # feedforward layer\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: of shape (batch_size, seq_len, input_size)\n",
    "            hx: of shape (batch_size, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = input.shape\n",
    "\n",
    "        if hx is None:\n",
    "            h0 = Variable(torch.zeros(self.num_layers,\n",
    "                                      batch_size,\n",
    "                                      self.hidden_size))\n",
    "        else:\n",
    "            h0 = hx\n",
    "        \n",
    "        # list of hidden states\n",
    "        h_list = []\n",
    "        outs = []\n",
    "        for i in range(self.num_layers):\n",
    "            h_list.append(h0[i, :, :])\n",
    "        \n",
    "        # for each time step\n",
    "        for t in range(seq_len):\n",
    "            for i in range(self.num_layers):\n",
    "                if i == 0:\n",
    "                    h_l = self.rnn_list[i](input[:, t, :], h_list[i])\n",
    "                else:\n",
    "                    h_l = self.rnn_list[i](h_list[i-1], h_list[i])\n",
    "                h_list[i] = h_l\n",
    "            outs.append(h_l)\n",
    "        # feedforward layer\n",
    "        output = self.linear(outs[-1].squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from Avi's code\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dls = DLStudio(\n",
    "                  dataroot = dataroot,\n",
    "                  path_saved_model = \"./saved_model\",\n",
    "                  momentum = 0.9,\n",
    "                  learning_rate =  1e-5,\n",
    "                  epochs = 1,\n",
    "                  batch_size = 1,\n",
    "                  classes = ('negative','positive'),\n",
    "                  use_gpu = True if torch.cuda.is_available() else False,\n",
    "              )\n",
    "\n",
    "text_cl = DLStudio.TextClassificationWithEmbeddings( dl_studio = dls )\n",
    "\n",
    "dataserver_train = DLStudio.TextClassificationWithEmbeddings.SentimentAnalysisDataset(\n",
    "                                 train_or_test = 'train',\n",
    "                                 dl_studio = dls,\n",
    "                                 dataset_file = dataset_archive_train,\n",
    "                                 path_to_saved_embeddings = path_to_saved_embeddings,\n",
    "                   )\n",
    "dataserver_test = DLStudio.TextClassificationWithEmbeddings.SentimentAnalysisDataset(\n",
    "                                 train_or_test = 'test',\n",
    "                                 dl_studio = dls,\n",
    "                                 dataset_file = dataset_archive_test,\n",
    "                                 path_to_saved_embeddings = path_to_saved_embeddings,\n",
    "                  )\n",
    "text_cl.dataserver_train = dataserver_train\n",
    "text_cl.dataserver_test = dataserver_test\n",
    "\n",
    "text_cl.load_SentimentAnalysisDataset(dataserver_train, dataserver_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_my_gru(net, name, device):\n",
    "    net = net.to(device)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr = dls.learning_rate, betas=(dls.momentum, 0.999))\n",
    "    training_loss_tally = list()\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "    loss_flag = 1e32\n",
    "    for epoch in range(dls.epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(text_cl.train_dataloader):\n",
    "            # get a sample from the train loader\n",
    "            review_tensor = data['review']\n",
    "            sentiment = data['sentiment']\n",
    "\n",
    "            review_tensor = review_tensor.to(device)\n",
    "            sentiment = sentiment.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = net(review_tensor)\n",
    "            output = softmax(output)\n",
    "            loss = criterion(output, torch.argmax(sentiment))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if i % 10 == 9:\n",
    "                row = [epoch, i, running_loss / 10]\n",
    "                with open(f'./solutions/{name}_training_log.csv', 'a') as csvFile:\n",
    "                    writer = csv.writer(csvFile)\n",
    "                    writer.writerow(row)\n",
    "                if running_loss < loss_flag:\n",
    "                    loss_flag = running_loss\n",
    "                    torch.save(net.state_dict(), f'./solutions/{name}_best_model.pt')\n",
    "                    \n",
    "                running_loss = 0.0\n",
    "    # return training_loss_tally\n",
    "\n",
    "net = RNN_custom(300, 512, 2, True, 2)\n",
    "train_with_my_gru(net, 'my_gru', device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece60146",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
