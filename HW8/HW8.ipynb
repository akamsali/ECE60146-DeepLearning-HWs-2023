{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from train import Train\n",
    "# from model import RNN_custom\n",
    "# dataroot = \"/Users/akshita/Documents/Acads/data/sentiment_analysis/\"\n",
    "# path_to_saved_embeddings = \"/Users/akshita/Documents/Acads/data/word2vec\"\n",
    "\n",
    "# train = Train(dataroot, path_to_saved_embeddings=path_to_saved_embeddings)\n",
    "# net = RNN_custom(300, 512, 2, True, 2)\n",
    "# name = \"RNN_custom\"\n",
    "# train.train(net, name, data_size=400, lr=1e-5, momentum=0.9, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLStudio import DLStudio\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn as nn \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"/Users/akshita/Documents/Acads/data/sentiment_analysis/\"\n",
    "dataset_archive_train = \"sentiment_dataset_train_400.tar.gz\"\n",
    "dataset_archive_test = \"sentiment_dataset_test_400.tar.gz\"\n",
    "path_to_saved_embeddings = \"/Users/akshita/Documents/Acads/data/word2vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRU(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size, \n",
    "                 bias=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: size of input vectors\n",
    "            hidden_size: size of hidden state vectors\n",
    "            bias: whether to use bias parameters or not\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, 3 * hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize all weights uniformly in the range [-1/sqrt(n), 1/sqrt(n)]\n",
    "        # n = hidden_size\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: of shape (batch_size, input_size)\n",
    "            hx: of shape (batch_size, hidden_size)\n",
    "\n",
    "        Returns:    \n",
    "            hy: of shape (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        if hx is None:\n",
    "            hx = Variable(input.new_zeros(input.size(0), self.hidden_size))\n",
    "\n",
    "        # Compute x_t and h_t\n",
    "        x_t = self.x2h(input)\n",
    "        h_t = self.h2h(hx)\n",
    "\n",
    "        # we split the output of the linear layers into 3 parts\n",
    "        # each of size hidden_size\n",
    "        x_reset, x_upd, x_new = x_t.chunk(3, 1)\n",
    "        h_reset, h_upd, h_new = h_t.chunk(3, 1)\n",
    "\n",
    "        # compute the reset, update and new gates\n",
    "        reset_gate = torch.sigmoid(x_reset + h_reset)\n",
    "        update_gate = torch.sigmoid(x_upd + h_upd)\n",
    "        new_gate = torch.tanh(x_new + (reset_gate * h_new))\n",
    "\n",
    "        hy = update_gate * hx + (1 - update_gate) * new_gate\n",
    "\n",
    "        return hy\n",
    "    \n",
    "class RNN_custom(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size,\n",
    "                 num_layers, \n",
    "                 bias,\n",
    "                output_size\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # list of GRU cells\n",
    "        self.rnn_list = nn.ModuleList()\n",
    "        self.rnn_list.append(MyGRU(input_size, \n",
    "                                     hidden_size, \n",
    "                                     bias))\n",
    "        for i in range(num_layers-1):\n",
    "            self.rnn_list.append(MyGRU(hidden_size, \n",
    "                                         hidden_size, \n",
    "                                         bias))\n",
    "        # feedforward layer\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: of shape (batch_size, seq_len, input_size)\n",
    "            hx: of shape (batch_size, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = input.shape\n",
    "\n",
    "        if hx is None:\n",
    "            h0 = Variable(torch.zeros(self.num_layers,\n",
    "                                      batch_size,\n",
    "                                      self.hidden_size))\n",
    "        else:\n",
    "            h0 = hx\n",
    "        \n",
    "        # list of hidden states\n",
    "        h_list = []\n",
    "        outs = []\n",
    "        for i in range(self.num_layers):\n",
    "            h_list.append(h0[i, :, :])\n",
    "        \n",
    "        # for each time step\n",
    "        for t in range(seq_len):\n",
    "            for i in range(self.num_layers):\n",
    "                if i == 0:\n",
    "                    h_l = self.rnn_list[i](input[:, t, :], h_list[i])\n",
    "                else:\n",
    "                    h_l = self.rnn_list[i](h_list[i-1], h_list[i])\n",
    "                h_list[i] = h_l\n",
    "            outs.append(h_l)\n",
    "        # feedforward layer\n",
    "        output = self.linear(outs[-1].squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from Avi's code\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dls = DLStudio(\n",
    "                  dataroot = dataroot,\n",
    "                  path_saved_model = \"./saved_model\",\n",
    "                  momentum = 0.9,\n",
    "                  learning_rate =  1e-5,\n",
    "                  epochs = 1,\n",
    "                  batch_size = 1,\n",
    "                  classes = ('negative','positive'),\n",
    "                  use_gpu = True if torch.cuda.is_available() else False,\n",
    "              )\n",
    "\n",
    "text_cl = DLStudio.TextClassificationWithEmbeddings( dl_studio = dls )\n",
    "\n",
    "dataserver_train = DLStudio.TextClassificationWithEmbeddings.SentimentAnalysisDataset(\n",
    "                                 train_or_test = 'train',\n",
    "                                 dl_studio = dls,\n",
    "                                 dataset_file = dataset_archive_train,\n",
    "                                 path_to_saved_embeddings = path_to_saved_embeddings,\n",
    "                   )\n",
    "dataserver_test = DLStudio.TextClassificationWithEmbeddings.SentimentAnalysisDataset(\n",
    "                                 train_or_test = 'test',\n",
    "                                 dl_studio = dls,\n",
    "                                 dataset_file = dataset_archive_test,\n",
    "                                 path_to_saved_embeddings = path_to_saved_embeddings,\n",
    "                  )\n",
    "text_cl.dataserver_train = dataserver_train\n",
    "text_cl.dataserver_test = dataserver_test\n",
    "\n",
    "text_cl.load_SentimentAnalysisDataset(dataserver_train, dataserver_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNI_GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1) -> None:\n",
    "        super(UNI_GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = torch.zeros(self.num_layers, x.size(1), self.hidden_size).requires_grad_()\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, h = self.gru(x, h.detach())\n",
    "        # print(out.shape)\n",
    "        out = self.fc(self.relu(out[:, -1]))\n",
    "        out = self.logsoftmax(out)\n",
    "        return out, h\n",
    "    \n",
    "\n",
    "class BI_GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.zeros(2*self.num_layers, x.size(1), self.hidden_size).requires_grad_()\n",
    "        # print(h.shape)\n",
    "        out, h = self.gru(x,h)\n",
    "        print(out.shape, out[:, -1].shape)\n",
    "        out = self.fc(self.relu(out[:, -1]))\n",
    "        out = self.logsoftmax(out)\n",
    "        return out, h\n",
    "    \n",
    "net_uni = UNI_GRU(300, 512, 2, 1)\n",
    "net_bi = BI_GRU(300, 512, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, dataloader, name='uni', epochs=1):\n",
    "    net = net.to(device)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-5, betas=(0.9, 0.999))\n",
    "    print(\"training\")\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in tqdm(enumerate(dataloader)):\n",
    "            review_tensor = data['review']\n",
    "            sentiment = data['sentiment']\n",
    "            review_tensor = review_tensor.to(device)\n",
    "            sentiment = sentiment.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = net(review_tensor)\n",
    "            loss = criterion(output, torch.argmax(sentiment, dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "\n",
    "train(net_uni, text_cl.train_dataloader, name='uni', epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_my_gru(net, name, device):\n",
    "    net = net.to(device)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr = dls.learning_rate, betas=(dls.momentum, 0.999))\n",
    "    training_loss_tally = list()\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "    loss_flag = 1e32\n",
    "    for epoch in range(dls.epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(text_cl.train_dataloader):\n",
    "            # get a sample from the train loader\n",
    "            review_tensor = data['review']\n",
    "            sentiment = data['sentiment']\n",
    "\n",
    "            review_tensor = review_tensor.to(device)\n",
    "            sentiment = sentiment.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = net(review_tensor)\n",
    "            output = softmax(output)\n",
    "            loss = criterion(output, torch.argmax(sentiment))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if i % 10 == 9:\n",
    "                row = [epoch, i, running_loss / 10]\n",
    "                with open(f'./solutions/{name}_training_log.csv', 'a') as csvFile:\n",
    "                    writer = csv.writer(csvFile)\n",
    "                    writer.writerow(row)\n",
    "                if running_loss < loss_flag:\n",
    "                    loss_flag = running_loss\n",
    "                    torch.save(net.state_dict(), f'./solutions/{name}_best_model.pt')\n",
    "                    \n",
    "                running_loss = 0.0\n",
    "    # return training_loss_tally\n",
    "\n",
    "net = RNN_custom(300, 512, 2, True, 2)\n",
    "train_with_my_gru(net, 'my_gru', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import csv\n",
    "def plot_training_loss(training_loss_tally, name):\n",
    "    plt.figure()\n",
    "    plt.plot(training_loss_tally)\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(f'./solutions/{name}_training_loss.png')\n",
    "\n",
    "for m in ['uni', 'bi', 'my_gru']:\n",
    "    with open(f'./solutions/{m}_training_log.csv', 'r') as csvFile:\n",
    "        reader = csv.reader(csvFile)\n",
    "        training_loss_tally = [float(row[2]) for row in reader]\n",
    "    plot_training_loss(training_loss_tally, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def validation(net, text_cl, name):\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     net = net.to(device)\n",
    "#     net.eval()    \n",
    "#     true = []\n",
    "#     pred = []\n",
    "#     with torch.no_grad():\n",
    "#         for i, data in enumerate(text_cl.test_dataloader):\n",
    "#             # get a sample from the test loader\n",
    "#             review_tensor, sentiment = data['review'], data['sentiment'] \n",
    "            \n",
    "#             # send review and sentiment tensor to cuda\n",
    "#             review_tensor = review_tensor.to(device)\n",
    "#             sentiment = sentiment.to(device)\n",
    "\n",
    "#             output = net(review_tensor)\n",
    "#             pred.append(torch.argmax(output).item())\n",
    "#             true.append(torch.argmax(sentiment).item())\n",
    "#         pickle.dump(true, open(f\"./solutions/{name}_true.pkl\", \"wb\"))\n",
    "#         pickle.dump(pred, open(f\"./solutions/{name}_pred.pkl\", \"wb\"))\n",
    "\n",
    "#     return true, pred\n",
    "\n",
    "def validate_and_conf_matrix(t, p, categories, name=\"Net\") -> None:\n",
    "    cm = confusion_matrix(t, p)\n",
    "    print(cm)\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=cm, xticklabels=categories, yticklabels=categories, fmt=\"g\")\n",
    "    plt.title(f\"Confusion matrix for {name}, accuracy={accuracy_score(p, t)}\")\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.savefig(f\"./solutions/cm_{name}.png\")\n",
    "\n",
    "\n",
    "path = \"./solutions/\"\n",
    "for model in [\"my\"]:\n",
    "    t = pickle.load(open(f\"{path}{model}_gru_true.pkl\", \"rb\"))\n",
    "    p = pickle.load(open(f\"{path}{model}_gru_pred.pkl\", \"rb\"))\n",
    "    validate_and_conf_matrix(t, p, [\"negative\", \"positive\"], name=f\"{model}_gru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_conf_matrix(cm, categories, name=\"Net\") -> None:\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=cm, xticklabels=categories, yticklabels=categories, fmt=\"g\")\n",
    "    plt.title(f\"Confusion matrix for {name}, accuracy={accuracy_score(p, t)}\")\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.savefig(f\"./solutions/cm_{name}.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece60146",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
