{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a building-block class that I have used in several networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, downsample=False, skip_connections=True):\n",
    "        super(SkipBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.skip_connections = skip_connections\n",
    "        \n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "\n",
    "        self.convo1 = nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1)\n",
    "        self.convo2 = nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        if downsample:\n",
    "            self.downsampler = nn.Conv2d(in_ch, out_ch, 1, stride=2)\n",
    "    def forward(self, x):\n",
    "        identity = x                                     \n",
    "        out = self.convo1(x)                              \n",
    "        out = self.bn1(out)                              \n",
    "        out = torch.nn.functional.relu(out)\n",
    "        if self.in_ch == self.out_ch:\n",
    "            out = self.convo2(out)                              \n",
    "            out = self.bn2(out)                              \n",
    "            out = torch.nn.functional.relu(out)\n",
    "        if self.downsample:\n",
    "            out = self.downsampler(out)\n",
    "            identity = self.downsampler(identity)\n",
    "        if self.skip_connections:\n",
    "            if self.in_ch == self.out_ch:\n",
    "                return out + identity                             \n",
    "            else:\n",
    "                out = torch.cat((out[:, :self.in_ch, :, :] + identity, out[:, self.in_ch:, :, :] + identity), dim=1)\n",
    "                return out\n",
    "\n",
    "class NetForYolo(nn.Module):\n",
    "    \"\"\"\n",
    "    Recall that each YOLO vector is of size 5+C where C is the number of classes.  Since C\n",
    "    equals 3 for the dataset used in the demo code in the Examples directory, our YOLO vectors\n",
    "    are 8 elements long.  A YOLO tensor is a tensor representation of all the YOLO vectors\n",
    "    created for a given training image.  The network shown below assumes that the input to\n",
    "    the network is a flattened form of the YOLO tensor.  With an 8-element YOLO vector, a\n",
    "    6x6 gridding of an image, and with 5 anchor boxes for each cell of the grid, the \n",
    "    flattened version of the YOLO tensor would be of size 1440.\n",
    "\n",
    "    In Version 2.0.6 of the RPG module, I introduced a new loss function for this network\n",
    "    that calls for using nn.CrossEntropyLoss for just the last C elements of each YOLO\n",
    "    vector. [See Lines 64 through 83 of the code for \"run_code_for_training_multi_instance_\n",
    "    detection()\" for how the loss is calculated in 2.0.6.]  Using nn.CrossEntropyLoss \n",
    "    required augmenting the last C elements of the YOLO vector with one additional \n",
    "    element for the purpose of representing the absence of an object in any given anchor\n",
    "    box of a cell.  \n",
    "\n",
    "    With the above mentioned augmentation, the flattened version of a YOLO tensor is\n",
    "    of size 1620.  That is the reason for the one line change at the end of the \n",
    "    constructor initialization code shown below.\n",
    "    \"\"\" \n",
    "    def __init__(self, skip_connections=True, depth=8):\n",
    "        super(NetForYolo, self).__init__()\n",
    "        # if depth not in [8,10,12,14,16]:\n",
    "        #     sys.exit(\"This network has only been tested for 'depth' values 8, 10, 12, 14, and 16\")\n",
    "        self.depth = depth // 2\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.bn1  = nn.BatchNorm2d(64)\n",
    "        self.bn2  = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.skip64_arr = nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.skip64_arr.append(SkipBlock(64, 64, skip_connections=skip_connections))\n",
    "        self.skip64ds =  SkipBlock(64,64, downsample=True, skip_connections=skip_connections)\n",
    "        self.skip64to128 =  SkipBlock(64, 128, skip_connections=skip_connections )\n",
    "        self.skip128_arr = nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.skip128_arr.append( SkipBlock(128,128, skip_connections=skip_connections))\n",
    "        self.skip128ds =  SkipBlock(128,128, downsample=True, skip_connections=skip_connections)\n",
    "        self.fc_seqn = nn.Sequential(\n",
    "            nn.Linear(128*16*16, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 64*5*9),\n",
    "            # nn.ReLU(inplace=True),\n",
    "#                    nn.Linear(2048, 1440)\n",
    "            # nn.Linear(2048, 1620)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))          \n",
    "        x = nn.MaxPool2d(2,2)(torch.nn.functional.relu(self.conv2(x)))       \n",
    "        for i,skip64 in enumerate(self.skip64_arr[:self.depth//4]):\n",
    "            x = skip64(x)                \n",
    "        x = self.skip64ds(x)\n",
    "        for i,skip64 in enumerate(self.skip64_arr[self.depth//4:]):\n",
    "            x = skip64(x)                \n",
    "        x = self.bn1(x)\n",
    "        x = self.skip64to128(x)\n",
    "        for i,skip128 in enumerate(self.skip128_arr[:self.depth//4]):\n",
    "            x = skip128(x)                \n",
    "        x = self.bn2(x)\n",
    "        x = self.skip128ds(x)\n",
    "        # print(\"first:\", x.shape)\n",
    "        x = x.view(-1,128*16*16)\n",
    "        # print(\"second:\", x.shape)\n",
    "        x = self.fc_seqn(x)\n",
    "        # print(\"third\", x.shape)\n",
    "        return x\n",
    "    \n",
    "ynet = NetForYolo(depth=2)\n",
    "\n",
    "# torchsummary.summary(ynet, (3, 256, 256))\n",
    "# len(ynet.parameters())\n",
    "# print(len(list(ynet.parameters())))\n",
    "\n",
    "# test = torch.randn(4, 3, 256, 256)\n",
    "# out = ynet(test)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def clip_file_names(a, location='mac', path='/Users/akshita/Documents/Acads/data/coco_custom_HW6'):\n",
    "    if location == 'vm':\n",
    "        return a\n",
    "    else:\n",
    "        img_path = a.split('/')[-3:]\n",
    "        return(os.path.join(path, *img_path))\n",
    "\n",
    "def plot_img(file, bboxes, category, name=test):\n",
    "    img = cv2.imread(clip_file_names(file))\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        x, y, w, h = bbox\n",
    "        img = cv2.circle(img, (bbox[2], bbox[3]), radius=1, color=(0, 225, 0), thickness=1)\n",
    "        # img = cv2.rectangle(\n",
    "        #     img, (x, y), (x + w, y + h), color=(36, 255, 0), thickness=2\n",
    "        # )\n",
    "        img = cv2.putText(\n",
    "            img,\n",
    "            org=(bbox[2], bbox[3]),\n",
    "            text= str(bbox[0]) + '_' + str(bbox[1]),\n",
    "            color=(255, 0, 0),\n",
    "            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=0.2,\n",
    "            thickness=1,\n",
    "        )\n",
    "\n",
    "    cv2.imwrite(f\"{name}.png\", img)\n",
    "\n",
    "categories = [\"bus\", \"cat\", \"pizza\"]\n",
    "total_train = []\n",
    "total_val = []\n",
    "for category in categories:\n",
    "    with open(f'./manifests/train_manifest_{category}.pkl', 'rb') as handle:\n",
    "        train_data = pickle.load(handle)\n",
    "    total_train.append(train_data)\n",
    "    handle.close()\n",
    "\n",
    "    with open(f'./manifests/val_manifest_{category}.pkl', 'rb') as handle:\n",
    "        train_data = pickle.load(handle)\n",
    "    total_val.append(train_data)\n",
    "    handle.close()\n",
    "\n",
    "# for i in range(3):\n",
    "#     print(len(total_train[i]), len(total_val[i]))\n",
    "\n",
    "bbox_count = [(0, 0) for _ in range(3)]\n",
    "for i in range(3):\n",
    "    for j in range(len(total_train[i])):\n",
    "        b_c = len(total_train[i][j]['bboxes'])\n",
    "        if b_c > bbox_count[i][1]:\n",
    "            bbox_count[i] = (j, b_c)\n",
    "# bbox_count\n",
    "bboxes = []\n",
    "files = []\n",
    "for i, j in enumerate(bbox_count):\n",
    "    bboxes.append(total_train[i][j[0]]['bboxes'])\n",
    "    files.append(total_train[i][j[0]]['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0327, 0.7552, 0.5780, 0.2596, 0.7384, 0.1397, 0.0993, 0.6724,\n",
       "          1.0000],\n",
       "         [0.9705, 0.5033, 0.0844, 0.3572, 0.6856, 0.2572, 0.1948, 0.2532,\n",
       "          0.0000]],\n",
       "\n",
       "        [[0.0178, 0.5296, 0.5119, 0.3904, 0.4527, 0.3145, 0.5785, 0.9956,\n",
       "          1.0000],\n",
       "         [0.1696, 0.0898, 0.7177, 0.1993, 0.3338, 0.9441, 0.7565, 0.8864,\n",
       "          1.0000]]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((2, 2, 8))\n",
    "b = torch.zeros((*a.shape[:-1], 1))\n",
    "c = torch.cat((a, b), dim=-1)\n",
    "\n",
    "# c[c[:, :, 0] < 1][:, -1]\n",
    "for i in range(c.shape[0]):\n",
    "    for j in range(c.shape[1]):\n",
    "        if c[i, j, 0] < 0.5:\n",
    "            c[i, j, -1] = 1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "from dataset import MyDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "categories = [\"bus\", \"cat\", \"pizza\"]\n",
    "\n",
    "batch_size = 4\n",
    "# train_data = MyDataset(categories=categories, split='train', manifest_path='./manifests', mac=True)\n",
    "val_data = MyDataset(categories=categories, split='val', manifest_path='./manifests', mac=True)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def validation(val_loader, ynet, yolo_interval=32):\n",
    "    device = \"cuda\" if torch.cuda.is_available() == True else \"cpu\"\n",
    "    print(device)\n",
    "    ynet = ynet.to(device)\n",
    "    ynet.eval()\n",
    "    predicted_bboxes = []\n",
    "    total_true_bboxes = []\n",
    "    total_val_classes = []\n",
    "    true_val_classes = []\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inp, t_l, true_bboxes = data\n",
    "            inp = inp.to(device)\n",
    "\n",
    "            out = ynet(inp)\n",
    "            out = out.view(-1, 64, 5, 9)\n",
    "\n",
    "            # get cells with top 5 highest values in the first element of the predicted yolo_vectors\n",
    "            # to achieve this, first we get the max value of anchor boxes for each cell\n",
    "            # then we sort the values in descending order and get the top 5 cells\n",
    "            pred_vals = out[:, :, :, 0]\n",
    "            pred_vals, _ = torch.max(pred_vals, dim=-1)\n",
    "            sorted_cells = torch.argsort(pred_vals, descending=True, dim=-1)\n",
    "\n",
    "            top5preds = torch.zeros((out.shape[0], 64, 9))\n",
    "\n",
    "            for i in range(out.shape[0]):\n",
    "                each_batch = out[i, sorted_cells[i]]\n",
    "                for j in range(each_batch.shape[0]):\n",
    "                    temp = each_batch[j, :, 0]\n",
    "                    args = torch.argmax(temp, dim=-1)\n",
    "                    top5preds[i, j] = each_batch[j, args]\n",
    "\n",
    "            pred_classes = top5preds[:, :, 5:-1]\n",
    "            pred_classes = nn.Softmax(dim=1)(pred_classes)\n",
    "            pred_classes = torch.argmax(pred_classes, dim=-1)\n",
    "\n",
    "            pred_regression_vec = top5preds[:, :, 1:5]\n",
    "            del_x, del_y = pred_regression_vec[:, :, 0], pred_regression_vec[:, :, 1]\n",
    "            h, w = pred_regression_vec[:, :, 2], pred_regression_vec[:, :, 3]\n",
    "\n",
    "            h *= yolo_interval\n",
    "            w *= yolo_interval\n",
    "            cell_row_index = torch.div(sorted_cells, 8, rounding_mode=\"floor\")\n",
    "            cell_col_index = sorted_cells % 8\n",
    "\n",
    "            yolo_offset = torch.ones_like(cell_row_index) * (yolo_interval / 2)\n",
    "            bb_center_x = (\n",
    "                cell_col_index * yolo_interval + yolo_offset + del_x * yolo_interval\n",
    "            )\n",
    "            bb_center_y = (\n",
    "                cell_row_index * yolo_interval + yolo_offset + del_y * yolo_interval\n",
    "            )\n",
    "\n",
    "            bb_top_left_x = bb_center_x - torch.div(w, 2, rounding_mode=\"floor\")\n",
    "            bb_top_left_y = bb_center_y - torch.div(h, 2, rounding_mode=\"floor\")\n",
    "\n",
    "            valid_preds = []\n",
    "            val_classes = []\n",
    "            for i in range(bb_top_left_x.shape[0]):\n",
    "                for j in range(bb_top_left_x.shape[1]):\n",
    "                    if bb_top_left_x[i, j] < 0:\n",
    "                        bb_top_left_x[i, j] = 0\n",
    "                    if bb_top_left_y[i, j] < 0:\n",
    "                        bb_top_left_y[i, j] = 0\n",
    "                    if (\n",
    "                        (h[i, j] > 256).any()\n",
    "                        or (w[i, j] > 256).any()\n",
    "                        or (w < 64).any()\n",
    "                        or (h < 64).any()\n",
    "                    ):\n",
    "                        continue\n",
    "                    valid_preds.append(\n",
    "                        torch.tensor(\n",
    "                            [bb_top_left_x[i, j], bb_top_left_y[i, j], w[i, j], h[i, j]]\n",
    "                        )\n",
    "                    )\n",
    "                    val_classes.append(pred_classes[i, j])\n",
    "\n",
    "            if len(valid_preds):\n",
    "                valid_preds = torch.stack(valid_preds, dim=-1)\n",
    "                predicted_bboxes.append(valid_preds)\n",
    "                total_true_bboxes.append(true_bboxes)\n",
    "                total_val_classes.append(val_classes)\n",
    "                true_val_classes.append(t_l)\n",
    "\n",
    "    if predicted_bboxes:\n",
    "        predicted_bboxes = torch.cat(predicted_bboxes, dim=-1)\n",
    "\n",
    "    return predicted_bboxes, total_true_bboxes, total_val_classes, true_val_classes\n",
    "\n",
    "predicted_bboxes = validation(val_loader, ynet)\n",
    "predicted_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 5])\n",
      "tensor([0.9274, 0.3575, 0.8599, 0.5772, 0.3570, 0.7029, 0.2207, 0.2920, 0.0407])\n",
      "tensor([0.9274, 0.3575, 0.8599, 0.5772, 0.3570, 0.7029, 0.2207, 0.2920, 0.0407])\n",
      "tensor([0.8180, 0.5276, 0.2137, 0.0457, 0.5645, 0.6096, 0.5926, 0.7152, 0.2464])\n",
      "tensor([0.8182, 0.0585, 0.9665, 0.9702, 0.4519, 0.2962, 0.7903, 0.0746, 0.4805])\n",
      "tensor([0.8182, 0.0585, 0.9665, 0.9702, 0.4519, 0.2962, 0.7903, 0.0746, 0.4805])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((4, 5, 5, 9))\n",
    "b = a[:, :, :, 0]\n",
    "print(b.shape)\n",
    "c = torch.argmax(b, dim=-1)\n",
    "d = a[0, c[0]]\n",
    "for i in range(5):\n",
    "    temp = d[i, :, 0]\n",
    "    args = torch.argmax(temp, dim=-1)\n",
    "    # print(args)\n",
    "    print(d[i, args])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "431it [16:49,  2.34s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# print(loss_CE)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m total_loss \u001b[39m=\u001b[39m loss_BCE \u001b[39m+\u001b[39m loss_MSE \u001b[39m+\u001b[39m loss_CE\n\u001b[0;32m---> 68\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     69\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     71\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m total_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import csv \n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() == True else \"cpu\"\n",
    "criterion1 = nn.BCELoss()                    # For the first element of the 8 element yolo vector              ## (3)\n",
    "criterion2 = nn.MSELoss()                    # For the regression elements (indexed 2,3,4,5) of yolo vector   ## (4)\n",
    "criterion3 = nn.CrossEntropyLoss()           # For the last three elements of the 8 element yolo vector        ## (5)\n",
    "sigmoid = nn.Sigmoid()\n",
    "softmax = nn.functional.softmax\n",
    "# print(\"\\n\\nLearning Rate: \", self.rpg.learning_rate)\n",
    "optimizer = optim.Adam(ynet.parameters(), lr=1e-4)                 ## (6)\n",
    "epochs = 2\n",
    "# total_loss = []\n",
    "logger = open('./solutions/test.csv', 'a')\n",
    "loss_flag = 1e32\n",
    "ynet = ynet.to(device)\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, d in tqdm(enumerate(train_loader)):\n",
    "        inp, _, true_yolo_aug = d\n",
    "        optimizer.zero_grad()\n",
    "        inp = inp.to(device)\n",
    "        true_yolo_aug = true_yolo_aug.to(device)\n",
    "\n",
    "        out = ynet(inp)\n",
    "        out = out.view(batch_size, 64, 5, -1)\n",
    "        # print(out.shape)\n",
    "        # pred_objectness = out[:, :, :, 0]\n",
    "        # regression_box = out[:, :, :, 1:5]\n",
    "        # pred_labels = out[:, :, :, 5:-1]\n",
    "        # print(objectness.shape)\n",
    "\n",
    "        # print(true_objectness.shape)\n",
    "        # present_obj = torch.where(true_yolo_aug[:, :, :, 0])#.unsqueeze(0)\n",
    "        present_obj = torch.nonzero(true_yolo_aug[:, :, :, 0])#.unsqueeze(0)\n",
    "\n",
    "        # print(present_obj)\n",
    "        pred_objectness = torch.zeros((len(present_obj)))\n",
    "        true_objectness = torch.ones((len(present_obj)))\n",
    "\n",
    "\n",
    "        pred_regression_box = torch.zeros((len(present_obj), 4))\n",
    "        true_regression_box = torch.zeros((len(present_obj), 4))\n",
    "\n",
    "        pred_labels = torch.zeros((len(present_obj)))\n",
    "        true_labels = torch.zeros((len(present_obj)))\n",
    "\n",
    "        for i, p in enumerate(present_obj):\n",
    "            # pred_objectness[i] = out[p[0], p[1], p[2], 0]\n",
    "            # print(p)\n",
    "            pred_regression_box[i] = out[p[0], p[1], p[2], 1:5]\n",
    "            true_regression_box[i] = true_yolo_aug[p[0], p[1], p[2], 1:5]\n",
    "\n",
    "            pred_labels[i] = torch.argmax(softmax(out[p[0], p[1], p[2], 5:-1], dim=0))\n",
    "            true_labels[i] = torch.argmax(true_yolo_aug[p[0], p[1], p[2], 5:-1])\n",
    "        \n",
    "        loss_BCE = criterion1(sigmoid(pred_objectness.unsqueeze(0)), true_objectness.unsqueeze(0))\n",
    "        # print(loss_BCE)\n",
    "\n",
    "        loss_MSE = criterion2(pred_regression_box.unsqueeze(0), true_regression_box.unsqueeze(0))\n",
    "        # print(loss_MSE)\n",
    "\n",
    "        loss_CE = criterion3(pred_labels.unsqueeze(0), true_labels.unsqueeze(0))\n",
    "        # print(loss_CE)\n",
    "\n",
    "        total_loss = loss_BCE + loss_MSE + loss_CE\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            data = [epoch + 1, i + 1, loss_BCE.item(), loss_MSE(), loss_CE.item(), running_loss / 100]\n",
    "            with logger:\n",
    "                write = csv.writer(logger)\n",
    "                write.writerow(data)\n",
    "\n",
    "            if running_loss < loss_flag:\n",
    "                loss_flag = running_loss\n",
    "\n",
    "                torch.save(ynet.state_dict(), \"./solutions/\" + 'net_name' + \".pt\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6, 2])\n",
      "tensor([[2, 3, 5, 0, 4],\n",
      "        [5, 2, 3, 0, 1],\n",
      "        [1, 3, 0, 2, 5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "a = torch.randn((3, 6, 2, 2))\n",
    "# print(a)\n",
    "b = a[:,:,:,0]\n",
    "print(b.shape)\n",
    "vals, args = torch.max(b, dim=-1)\n",
    "top5 = torch.argsort(vals, descending=True, dim=-1)[:, :5]\n",
    "print(top5)\n",
    "a[0, top5[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 5, 9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.view(4, 64, 5, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataset import MyDataset\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# categories = [\"bus\", \"cat\", \"pizza\"]\n",
    "# batch_size=4\n",
    "\n",
    "# train_dataset = MyDataset(categories=categories, split=\"train\")\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece60146",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
